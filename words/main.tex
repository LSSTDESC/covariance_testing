\RequirePackage{docswitch}
% \flag is set by the user, through the makefile:
%    make note
%    make apj
% etc.
\setjournal{\flag}

\documentclass[twocolumn]{\docclass}

%%%% Scott's macros
\newcommand{\sfig}[2]{
\includegraphics[width=#2]{#1}
        }
\newcommand{\Sfig}[2]{
    \begin{figure}[thbp]
    \sfig{../Figures/#1.pdf}{\columnwidth}
    \caption{{\small #2}}
    \label{fig:#1}
    \end{figure}
}
\newcommand{\Swide}[2]{
\begin{figure*}[thbp]
 \sfig{../Figures/#1.pdf}{.8\textwidth}
  \caption{{\small #2}}
   \label{fig:#1}
   \end{figure*}
}
\newcommand{\Sswide}[2]{
\begin{figure*}[thbp]
 \sfig{../Figures/#1.pdf}{.7\textwidth}
  \caption{{\small #2}}
   \label{fig:#1}
   \end{figure*}
}
\newcommand{\Svwide}[2]{
\begin{figure*}[thbp]
 \sfig{../Figures/#1.pdf}{\textwidth}
  \caption{{\small #2}}
   \label{fig:#1}
   \end{figure*}
}

\newcommand{\Spng}[2]{
    \begin{figure}[thbp]
    \sfig{../Figures/#1.png}{0.95\columnwidth}
    \caption{{\small #2}}
    \label{fig:#1}
    \end{figure}
}
\newcommand{\Rf}[1]{\ref{fig:#1}}
\newcommand{\rf}[1]{\ref{fig:#1}}
\newcommand{\ec}[1]{Eq.~(\ref{eq:#1})}
\newcommand{\ecalt}[1]{Eq.~\ref{eq:#1}}
\newcommand{\Ec}[1]{(\ref{eq:#1})}
\newcommand{\eeec}[3]{Eqs.~(\ref{eq:#1}, \ref{eq:#2}, \ref{eq:#3})}
\newcommand{\eql}[1]{\label{eq:#1}}
\newcommand\be{\begin{equation}}
\newcommand\ee{\end{equation}}
\def\bea{\begin{eqnarray}}
\def\eea{\end{eqnarray}}
% \def\bea{\begin{eqnarray}}
% \def\eea{\end{eqnarray}}
\def\svs{\nonumber\\}

% You could also define the document class directly
%\documentclass[]{emulateapj}

% Custom commands from LSST DESC, see texmf/styles/lsstdesc_macros.sty
\usepackage{lsstdesc_macros}

\usepackage{graphicx}
\graphicspath{{./}{./figures/}}
\bibliographystyle{apj}

% Add your own macros here:



% ======================================================================

\begin{document}

\title{Covariance Testing}

\maketitlepre

\begin{abstract}

There are a number of codes that compute covariance matrices analytically; the plan is to use these to build TJPCov. In this project, we start along the path of comparing these different codes, building up a suite of tools that can be used to compare covariance matrices. We expect these tools to be useful not only for converging on a single accurate code for computing covariance matrices but also more generally for understanding which parts of the covariance matrix carry the most information (and therefore need the most attention to get right) and which are not relevant (so for example matrices that are not positive definite may still be usable if the negative eigenmodes are not relevant).
\end{abstract}

% Keywords are ignored in the LSST DESC Note style:
\dockeys{}

\maketitlepost

% ----------------------------------------------------------------------
% {% raw %}

\section{Introduction}
\label{sec:intro}


% ----------------------------------------------------------------------

\section{Methods}
\label{sec:methods}

There are several ways to tests covariance matrices. In this work, we will illustrate these on cosmic shear statistics $\xi_\pm(\theta)$, focusing for the most part on the Year 1 results of the Dark Energy Survey~\citep{Abbott:2017wau} (DESY1), and also on predictions for Year 3 (DESY3).  For each $\xi_+(\theta)$ and $\xi_-(\theta)$, we have 200 datapoints, which give 400 in total, divided in 20 angular bins between 2.5 and 250 arcmin, and five tomographic bins, as described in ~\citep{Abbott:2018cms}. We apply the same angular cuts to this dataset as in the aforementioned paper, resulting in 227 datapoints, which correspond to a $227 \times 227$ covariance matrix.

The two codes employed here for computing the covariance matrices are {\tt Cosmolike}~\citep{Krause:2016jvl} and the one used to analyze the KiDS-450 survey~\citep{Kohlinger:2017sxk}, hereafter referred to as CL and BJ, respectively. To perform cosmological parameter inference we used the {\tt CosmoSIS}~\citep{ZUNTZ201545} pipeline, choosing the {\tt MultiNest}~\citep{nested:feroz09} sampler to explore the parameter space, with 500 livepoints, efficiency set to 0.8, and tolerance to 0.1.

\subsection{One-to-one Comparison}

This is a simple matter of comparing elements of a covariance matrix, usually starting with diagonal elements. \figref{xipmscatter} shows an example.

\begin{figure}
\includegraphics[width=0.9\columnwidth]{xipmscatter.png}
\caption{A simple scatter plot of elements of covariance matrices produced by two separate halo model codes. \label{fig:xipmscatter}}
\end{figure}


\subsection{Eigenvalues and eigenvectors}

An intuitive approach to identifying the most contributing elements of a covariance matrix is to look at its eigenvalues, since we can associated them to the noise in the measurement. The lowest eigenvalues would correspond to the least noise and therefore hold the most information about the parameters, whereas the highest eigenvalues would contribute significantly less. We check this assumption by replacing the 50 highest eigenvalues with a very large number (about nine orders of magnitude higher), thus removing their effective contribution, and then obtain a new covariance matrix with the modified eigenvalues. To check whether we have indeed kept the most informative modes, we perform an analysis with the new covariance matrix, to constrain cosmological and systematic parameters; which we then compare to the constraints obtained with the original covariance matrix. The results are shown in Fig.~\rf{eigenvalues}, for $\Omega_m$ and $\sigma_8$, where we can see that although the constraints are consistent, we have lost a noticeable amount of information of the parameters.

\begin{figure}
\includegraphics[width=0.9\columnwidth]{Eigenvalues.pdf}
\caption{Constraints on cosmological parameters $\Omega_m$ and $\sigma_8$ for the original DESY1 covariance matrix (in purple) and for a new covariance matrix (in blue) obtained by setting the fifty highest eigenvalues of the original matrix to nine orders of magnitude higher. The constraints are consistent, however, we notice a slightly larger $1\sigma$ error for the latter. \label{fig:eigenvalues}}
\end{figure}

A visual comparison of two matrices produced by CL and BJ is shown in Fig.~\rf{coveigen} where we have plotted their eigenvalues in blue and orange, respectively. On average, the eigenvalues differ by an order of $\approx 15 \%$. Notice that the order of magnitude of the values vary greatly, making it difficult to compare them efficiently; furthermore, it is not clear which elements are most important. 

\begin{figure}
\includegraphics[width=0.9\columnwidth]{coveigen_BJ-CL.png}
\caption{A plot showing the $227$ eigenvalues of two covariance matrices produced for cosmic shear for the Year 3 projection of the Dark Energy Survey. The blue curve is for CL and the orange one is for BJ. \label{fig:coveigen}}
\end{figure}

\figref{evector} shows an example of one of the eigenvectors, the one associated with the smallest eigenvalue. This low-eigenvalue mode picks up the differences between the correlation function at different angular scales (each vertical line delineates between two-point functions of shears in different tomographic bin pairs).

\begin{figure}
\includegraphics[width=0.9\columnwidth]{evector.png}
\caption{A simple scatter plot of elements of covariance matrices produced by two separate halo model codes. \label{fig:evector}}
\end{figure}

Another interesting way of analysing covariance matrices is by calculating the signal to noise ratio (S/N). A simple way of achieving this is to note that the total signal to noise squared is

%Since the values of the data points vary by many orders of magnitude, the range of eigenvalues in Fig.~\rf{evector} is a bit misleading. In addition, it is not clear which eigenvectors should be dropped: should it be chose with large eigenvalues (noise) or small ones (the noise is noisy)? Ideally, what we would like is to create a list of eigenvectors ordered in some way and drop the least important. This ties into the section below on Shrinkage. A very simple way to do this is to note that the total signal to noise squared is
\be
\left(\frac{S}{N}\right)^2 = \sum_{ij} D_i C^{-1}_{ij} D_j\
,\ee
where $D_i$ and $D_j$ are the data points and $C$ the covariance matrix. If $C$ were diagonal, then the eigenvectors would simply be the data points themselves, and we could estimate the signal to noise squared expected in each mode by simply computing $T_i^2/C_{ii}$ where $T_i$ is the theoretical prediction. Then we could throw out the modes with the lowest signal to noise. Since $C$ is not diagonal, we have to first diagonalize it and then order the values. So, we write the expected signal to noise squared as
%to do things a bit differently. Let's first define re-scaled data points
\bea
\left(\frac{S}{N}\right)^2 &=& \sum_{ij} T_i  C^{-1}_{ij} T_j\nonumber\\
&=& \sum_{i} \frac{v_i^2}{\lambda_i}
,\eea
where $\lambda_i$ are the eigenvalues of the covariance matrix, which is diagonalized with the unitary matrix $U$, and the eigenvectors are 
\be
v_i\equiv U_{ij}^t T_j
\ee
This makes it very clear which modes should be kept and which should be dropped. Modes $v_i$ for which $v_i^2/\lambda_i$ is very small can be discarted. 

We follow the same procedure described earlier for the eigenvalues, where we remove the lowest values by setting them to even lower values. When doing this, we must be careful, because the highest modes do not necessarily hold the most information for all of the parameters; it is therefore necessary to check this for each parameter before applying the cuts. To test this we take
\bea
\left(\frac{\partial S/\partial \theta}{N}\right)^2 = \sum_{i} \frac{(\partial v_i / \partial \theta)^2}{\lambda_i}
,\eea
with
\be
v_i^\theta \equiv U_{ij}^t \frac{\partial T_j}{\partial \theta}
.\ee

The importance of this is illustrated in Fig.~\rf{signalnoise}, where we can see that while the highest signal to noise for all the parameters does indeed correspond to those that hold most information for parameters $\Omega_m$ and $A_s$, this is not the case for the intrinsic alignment parameters $A$ and $\eta$, so that we end up losing constraining power over these parameters when we modify the lowest values of S/N. This is clearly shown by looking at Fig\textcolor{red}{insert figure}, which shows constraints for the aforementioned parameters for the modified S/N and compares them against the ones obtained for the original covariance matrix.

\begin{figure}
\includegraphics[width=0.9\columnwidth]{SN_cuts.pdf}
\caption{Scatter plot for the relation between the signal to noise obtained with the covariance matrix for DESY1 for each parameter (x-axis) against that for the full set of parameters (y-axis). The derivatives are shown with respect to cosmological parameters $\Omega_m$ (blue) and $A_s$ (orange), and for the intrinsic alignment parameters $A$ (green) and $\eta$ (red). The purple rectangle spreads until the fiftieth lowest value of S/N, which corresponds to the values that were modified for parameter constraints. \label{fig:signalnoise}}
\end{figure}




\subsection{Parameter Estimation}

Ultimately, what matters is how well the likelihood does at extracting parameter constraints. Since most analyses assume a Gaussian likelihood, this boils down to how well the contours in parameter space agree when computing the $\chi^2$ using two different covariance matrices.

\subsection{Shrinkage}

There have been several methods proposed in the literature to compress the data vectors, extracting as much information as possible. Here we consider two: first compression at the map level~\citep{Alonso:2017hhj}, where linear combination of the tomographic maps are used. If there are 4 tomographic bins, an uncompressed analysis would require ten separate 2-point functions (or 20 for cosmic shear), whereas a compression scheme leads to just a few uncorrelated maps. If there were 3 such maps, then only three 2-point functions would need to be used for the likelihood analysis.

We characterize a given element in the data vector by its angular and tomographic indices: $a_i = a_{lm,\alpha}$ where $l,m$ denote the indices corresponding to given spherical harmonics and $\alpha$ is a given tomographic bin, or equivalently $a_i = a_\alpha(\vec\theta)$ in real space. The compression occurs in terms of tomographic bins, so that 
\be
b_\mu(\vec\theta) = \sum_{\alpha} F_{\mu\alpha} a_\alpha(\theta)
\ee
where the $F$'s are chosen \footnote{Note that this definition of $F$ differs from that in \citep{Alonso:2017hhj} in that it includes the inverse of the noise matrix.} so that the correlation functions of the $b$'s are diagonal in tomographic space: 
\bea
w_{\mu\nu}(\theta) &=& \langle b_\mu(\vec\theta_1) b_\nu(\vec\theta_2)\rangle\vert_{\vert\vec\theta_1-\vec\theta_2\vert\in\theta}
\svs
&=& \delta_{\mu,\nu} w_{\mu}(\theta).
\eea
Usually with $N_t$ tomographic bins, one must consider $N_t(N_t+1)/2$ correlation functions, but -- due to the transformation that diagonalizes the elements -- there are only $N_t$ correlation functions to consider (for a given $\theta$). Even better, these can be ordered by the information they contain, so fewer than $N_t$can be used. In the example used in ~\citep{Alonso:2017hhj}, 16 tomographic bins were assumed, so that the standard treatment would require 136 correlation functions, but only 3 were needed in order to extract accurate constraints. 

This method suggests a way of extracting the most important pieces of the full covariance matrix $C$. We simply compute the covariance matrix of the $w_{\mu}$ in terms of $C$ and keep only the most important terms. That is,
\bea
C^b_{\mu\nu} &\equiv& \langle (w_{\mu} -\bar w_\mu)\,(w_{\nu} -\bar w_\nu)\rangle
\svs
&=& \sum_{\alpha\alpha'\beta\beta'} F_{\mu\alpha}F_{\mu\alpha'}\, F_{\nu\beta}F_{\nu\beta'}\, C_{\alpha\alpha'\beta\beta'} .
\eea
Here the angular indices have been suppressed (there are two of them, one for each $w_\mu$), but the full tomographic complexity has been retained. The covariance matrix on the right includes a total of $N_t^2 \times N_t^2$ terms (some of which are equal because of symmetry) corresponding to all possible pairs of two-point functions. But $C^b$ on the right contains only $N_t^2$ elements, and again these are ordered, so we can use only a subset of them. In the simplest case, where only one linear combination is needed so $\mu=\nu=1$, a single number (for each pair of angular bins) captures all the relevant information from the full covariance matrix.

The second compression takes place at the 2-point level~\citep{Zablocki:2015zcm}, with the compressed data vector containing linear combinations of the many 2-point functions. In principle, this might work with only $N_p$ 2-point functions where $N_p$ is the number of parameters varied, and each mode, or linear combination, contains all the information necessary about the parameter of interest. 

For each parameter $p_\alpha$ that is varied, one captures a single linear mode
\be
y_\alpha = U_{\alpha,i} D_i
\ee
where $D_i$ are the data points and the coefficients are defined as
\be
U_{\alpha,i} \equiv \frac{\partial T_j}{\partial p_\alpha} \, C^{-1}{}_{ji}
\ee
where $T_j$ is the theoretical prediction for the data point $D_j$.
The now much smaller data set $\{y_\alpha\}$, which contains as few as $N_p$ data points carries with it its own covariance matrix, with which the $\chi^2$ can be computed for each point in parameter space. Propagating through shows that this covariance matrix is related to the original $C_{ij}$ via
\be
C_{\alpha\beta} = U_{\alpha i} C_{ij} U^t_{j\beta}.
\ee
To take an example from DES-Y1, $C_{ij}$ is a $400\times 400$ matrix, while the number of parameters needed to specify the model is only 16, so $C_{\alpha\beta}$ is a $16\times 16$ matrix. We have apparently captured from the initial set of $400\times 401/2=80,200$ independent elements of the covariance matrix a small subset (only 136 in this case) of linear combinations of these 80k elements that really matter. If two covariance matrices give the same set of $C_{\alpha\beta}$, we do not care whether any of the other eighty thousand elements differ from one another.

When comparing the constraints obtained via the compressed dataset and corresponding covariance matrix to that obtained with the full, original ones, we must be careful when computing $U_{\alpha, i}$. This is because the curve shifts depending on where the derivative is taken. For better results, we must take the derivative at the best-fit value, otherwise the constraints will shift, as discussed in more detail in Appendix A. The power of this compression scheme can be appreciated in Fig.

We apply this methodology to comparing the covariance matrices of interest, i. e. CL and BJ. In order to do this, we take two different approaches: first, we assume that $U_{\alpha, i}$ is the same for both covariance matrices and we calculate it with BJ. The second approach is that each compression scheme must use the original covariance matrix that will be compressed, so that $U_{\alpha, i}$ will be different for each covariance matrix. Fig.~\rf{correlation} was obtained for the first method, it shows the correlation matrix for BJ and CL, and that of the difference between them; this figure is important because we can clearly see the difference between the two matrices by simply looking at only $16 \times 16$ elements, as opposed to having to analyse the larger correlation matrix for the full covariance matrices. It is also crucial that the matrices used for comparison here are those obtained via the same compression scheme, so that we can be sure that their differences are indeed only related to the differences in the original $227 \times 227$ matrices.

\begin{figure}
\includegraphics[width=0.9\columnwidth]{Correlation_compression.png}
\caption{The upper right and lower left plots display the correlation matrix for BJ and CL respectively, while the lower right is the difference between the two. \label{fig:correlation}}
\end{figure}


% ----------------------------------------------------------------------

\section{Results}
\label{sec:results}



% ----------------------------------------------------------------------

\section{Discussion}
\label{sec:discussion}



% ----------------------------------------------------------------------

\section{Conclusion}
\label{sec:conclusion}



% ----------------------------------------------------------------------

\subsection*{Acknowledgments}

%%% Here is where you should add your specific acknowledgments, remembering that some standard thanks will be added via the \code{desc-tex/ack/*.tex} and \code{contributions.tex} files.

%This paper has undergone internal review in the LSST Dark Energy Science Collaboration. % REQUIRED if true

\input{contributions} % Standard papers only: author contribution statements. For examples, see http://blogs.nature.com/nautilus/2007/11/post_12.html

% This work used TBD kindly provided by Not-A-DESC Member and benefitted from comments by Another Non-DESC person.

% Standard papers only: A.B.C. acknowledges support from grant 1234 from ...

\input{desc-tex/ack/standard} % also available: key standard_short

% This work used some telescope which is operated/funded by some agency or consortium or foundation ...

% We acknowledge the use of An-External-Tool-like-NED-or-ADS.

%{\it Facilities:} \facility{LSST}

% Include both collaboration papers and external citations:
\bibliography{main,lsstdesc}

\end{document}

% ======================================================================

\RequirePackage{docswitch}
% \flag is set by the user, through the makefile:
%    make note
%    make apj
% etc.
\setjournal{\flag}

\documentclass[twocolumn]{\docclass}

%%%% Scott's macros
\newcommand{\sfig}[2]{
	\includegraphics[width=#2]{#1}
}
\newcommand{\Sfig}[2]{
	\begin{figure}[thbp]
		\sfig{../figures/#1.pdf}{\columnwidth}
		\caption{{\small #2}}
		\label{fig:#1}
	\end{figure}
}
\newcommand{\Swide}[2]{
	\begin{figure*}[thbp]
		\sfig{../figures/#1.pdf}{.8\textwidth}
		\caption{{\small #2}}
		\label{fig:#1}
	\end{figure*}
}
\newcommand{\Sswide}[2]{
	\begin{figure*}[thbp]
		\sfig{../figures/#1.pdf}{.7\textwidth}
		\caption{{\small #2}}
		\label{fig:#1}
	\end{figure*}
}
\newcommand{\Svwide}[2]{
	\begin{figure*}[thbp]
		\sfig{../figures/#1.pdf}{\textwidth}
		\caption{{\small #2}}
		\label{fig:#1}
	\end{figure*}
}

\newcommand{\Spng}[2]{
	\begin{figure}[thbp]
		\sfig{../figures/#1.png}{0.95\columnwidth}
		\caption{{\small #2}}
		\label{fig:#1}
	\end{figure}
}
\newcommand{\Rf}[1]{Figure \ref{fig:#1}}
\newcommand{\rf}[1]{Figure \ref{fig:#1}}
\newcommand{\rsec}[1]{\S\ref{sec:#1}}
\newcommand{\rssec}[1]{\S\ref{subsec:#1}}
\newcommand{\ec}[1]{Eq.~(\ref{eq:#1})}
\newcommand{\ecalt}[1]{Eq.~\ref{eq:#1}}
\newcommand{\Ec}[1]{(\ref{eq:#1})}
\newcommand{\eeec}[3]{Eqs.~(\ref{eq:#1}, \ref{eq:#2}, \ref{eq:#3})}
\newcommand{\eql}[1]{\label{eq:#1}}
\newcommand\be{\begin{equation}}
\newcommand\ee{\end{equation}}
\def\bea{\begin{eqnarray}}
\def\eea{\end{eqnarray}}
\def\svs{\nonumber\\}

% You could also define the document class directly
%\documentclass[]{emulateapj}

% Custom commands from LSST DESC, see texmf/styles/lsstdesc_macros.sty
\usepackage{lsstdesc_macros}
\newcommand\scott[1]{{\bf [Scott: #1]}}
\newcommand\tassia[1]{{\bf [Tassia: #1]}}
\newcommand\tq[1]{{\bf [Tq: #1]}}
\usepackage{graphicx}
\graphicspath{{./}{./figures/}}
\bibliographystyle{apj}

%\usepackage{lineno}
%\linenumbers

% Add your own macros here:

% ======================================================================

\begin{document}
	
	\title{Data Compression and Covariance Matrix Inspection: Cosmic Shear}
	\maketitlepre
	
	\begin{abstract}
	   Covariance matrices are among the most difficult pieces of end-to-end cosmological analyses. In principle, each element involves a four-point function, and there are often hundreds of thousands of elements. How best to check these crucial ingredients? Here, we investigate various compression mechanisms, which vastly reduce the size of the covariance matrices in the context of cosmic shear statistics. 
	  This helps identify the parts of the covariance matrix that are most significant to parameter estimation. We start with simple ways of compression, by isolating and ``removing" 200 modes associated with the highest eigenvalues, then those with the lowest signal-to-noise ratio, before moving on to compression at the tomographic level and, finally, with the Massively Optimized Parameter Estimation and Data (MOPED) algorithm. We find that, while most of these methods proved useful for a few parameters of interest, like $\Omega_m$, the simplest lose constraining power on the intrinsic alignment (IA) parameters as well as $S_8$. %The former was accomplished using a method based on the Karhunen-Lo\'eve (KL) decomposition to obtain the modes with highest SNR, and we show that, in order to reproduce compatible results, we need at least two KL-modes. 
For the case considered --- cosmic shear from the first year of data from the Dark Energy Survey --- only MOPED was able to reproduce the original constraints in the 16-parameter space. Finally, we apply a tolerance test to the elements of the compressed covariance matrix obtained with MOPED and confirm that the intrinsic alignment parameter $A_{\mathrm{IA}}$ is the most sensitive to changes in the covariance matrix.
	
		%In this paper, we start along the path of comparing covariance matrices for cosmic shear statistics generated by two different codes. The main goal is to identify the parts of the covariance matrix that are most significant to parameter estimation, and therefore which ones should be calculated more accurately. We engage different ways of doing this, starting with a simple one-to-one comparison of the elements, then moving on to eigenvalues and finally to the signal-to-noise ratio (SNR). In the spirit of reducing the number of relevant elements, we “remove” 200 modes associated with the highest eigenvalues, then those with the lowest SNR. We find that it is not possible to locate the most important elements using the first method but, while the analysis with the SNR proved resourceful for a few parameters of interest, like $\Omega_m$, we lost constraining power on the intrinsic alignment (IA) parameters as well as $S_8$. We also tested ways to shrink the covariance matrix, both at the tomographic level, and for the two-point functions. The former was accomplished using a method based on the Karhunen-Lo\'eve (KL) decomposition to obtain the modes with highest SNR, and we show that, in order to reproduce compatible results, we need at least two KL-modes, but just like in the SNR case, this is not the case for all parameters. Finally, we apply a lossless compression scheme to the covariance matrix, capable of reducing the dimension to the number of free parameters. We were successful in reproducing the constraints obtained previously and show that the elements of the new matrix have an error tolerance of up to $25\%$.
	\end{abstract}
	
	% Keywords are ignored in the LSST DESC Note style:
	\dockeys{}
	
	\maketitlepost
	
	% ----------------------------------------------------------------------
	% {% raw %}
	
	\section{Introduction}
	\label{sec:introduction}
	
	Cosmic shear is a weak lensing effect caused by the large-scale structure of the universe and is an important tool for constraining cosmology. The most common way of extracting information about cosmology from cosmic shear is to use two-point functions; in that case, an analysis requires a covariance matrix. For a two-point data vector of length $N$, the covariance matrix is a symmetric $N\times N$ matrix with $N\times (N+1)/2$ individual elements that capture the auto and cross-correlation of the data vector. As the data vector increases, the number of elements in the covariance matrix grows quadratically and becomes harder to analyse. One could potentially speed up computations and provide a simpler method of analysing the covariance matrix by using compression schemes capable of significantly reducing the size of the matrix while still retaining relevant information about the parameters of interest. One way of obtaining this is to use the Massively Optimized Parameter Estimation and Data (MOPED) algorithm, in which, if the noise in the data does not depend on the model parameters, the compression is truly lossless \cite{Heavens:2000hjl, Tegmark:1997maa}. MOPED has been widely used in literature for a variety of topics, like, for example, analysing CMB data \cite{Zablocki:2015zcm}, for redshift space galaxy power spectrum and bispectrum \cite{Gualdi:2018mjl}, for parameter-dependent covariance matrices \cite{Heavens:2017smv}, and, more recently, for compressing the Planck 2015 temperature likelihood \cite{Heather:2019d}.
	
	We will focus on cosmic shear measurements from the Dark Energy Survey (DES)~\cite{Troxel:2017xyo} Year 1 release; the data vector has 227 elements, varying with angular separation, and different pairs of tomographic redshift bins. Since our parameter space consists of 16 free parameters, we can use MOPED to reduce the $25,878$ independent elements of the covariance matrix, to only $136$.
	
	Apart from MOPED, we will also be analyzing the covariance matrix with three other compression methods: the first involves discarding the modes associated with the highest eigenvalues; the second method removes those with the lowest signal-to-noise ratio. In an effort to shrink the covariance matrix to about $10\%$ of its original size, we remove, in both cases, 200 such modes.
	
	Finally, the third method consists of a map-level compression \citep{Alonso:2017hhj}, where linear combinations of the tomographic maps are used to retain as much information as possible. Compression of the tomographic bin pairs then considerably reduces the size of the data vector of the two-point functions. For example, we will see that most of the information in the four tomographic bins used by DESY1 can be compressed into a single linear combination of those bins. Therefore, instead of $(4\times5)/2$ two-point functions for each angular bin, we need include only one. %: the compression in the size of the data vector by a factor of ten means that there are a hundred fewer elements of the covariance matrix to study.
    For this purpose, the tomographic bins will have the same length, and so the angular cuts to the dataset and covariance matrix will be different from the ones used in the aforementioned DESY1 paper. The chosen covariance matrix has a dimension of $190 \times 190$, and so, for one eigenmode, the compressed matrix will have 190 independent elements.
	
	%There are multiple codes that are able to generate the cosmic shear covariance matrix. In particular, we are considering two of them \citep{Krause:2016jvl} \citep{Kohlinger:2017sxk}, which will be discussed in detail in \rsec{methods}.
	
\scott{not sure this adds anything; may want to remove.}	The main goals of this paper are as follows:
	\begin{itemize}
	\item Among the vast number of elements in the covariance matrix, identify the most important ones. 	
	\item Apply and test compression schemes on the DESY1 Cosmic Shear data.
	%\item Compare the original and compressed covariance matrices.
	%\item  Explore methods to shrink the covariance matrix. % once identify the more important elements, using the technique developed by Alonso \citep{Alonso:2017hhj} and by Zablocki and Dodelson \citep{Zablocki:2015zcm}.	
	\item Introduce noise to the elements in order to test their tolerance by quantifying responses in the likelihood analysis.
	\end{itemize}
	
	%We accomplish these by considering several methods of identifying the elements of the covariance matrix that are most relevant. %Although we will consider only the one example of DES cosmic shear measurements and projections, we think that our conclusions will be generalised. 
	In \rsec{methods}, we start by describing the dataset and the covariance matrices used. We then proceed to review each compression scheme and apply each to DESY1 cosmic shear.  The ultimate test is how well each reproduces the constraints obtained with the full covariance matrix. %We validate our findings by comparing them to the constraints obtained with the unmodified covariance matrix. 
We follow up by showing that compression can be a useful tool to compare two different covariance matrices, in \rsec{comparison_matrices}.%, starting with the original ones, and then moving on to the compressed scenario. 
Our tolerance test is described in \rsec{tolerance}, where we test what happens to the parameter constraints when we introduce noise to elements and eigenvalues separately. Finally, our conclusions are summarised in \rsec{conclusion}.
	
	%The next three sections walk through increasingly complex ways of determining the most important parts of these matrices: we start with an element by element comparison of  the two covariance matrices in \rssec{comparison_one-one}, then move on to look at the eigenvalues in \rsec{eigenvalues}, and, finally we use the signal-to-noise ratio in \rsec{snr}. We validate our findings by comparing them with the constraints obtained with the unmodified covariance matrix. %In \rssec{comparison_one-one}, we compare element by element of the two covariance matrices . In \rsec{eigenvalues},  we use eigenvalue as a way to compare the covariance matrices. In \rsec{snr}, we calculate the signal-to-noise ratio and use it to compare the covariance matrices. %We will use this metric a bit throughout but want to  here that our goals is to simply finding out that two covariance matrices give different results is a black-box approach. One cannot identify the source of the disagreement. The methods described here aim to be a bit deeper to try to understand where the key differences arise and which differences are most important. (this is ambiguous)
	
	%A simple compression method is applied in both \rsec{eigenvalues} and \rsec{sec:snr}, where we discard 200 modes of the covariance matrix associated with the highest eigenvalues, then those with lowest signal-to-noise ratio, respectively. We apply more complicated compression schemes in  \rsec{sec:shrinkage}, using techniques developed by Alonso \citep{Alonso:2017hhj} and by Tegmark and Heavens \citep{Tegmark:1997maa}, capable of reducing the dimension to the number of free parameters, thus obtaining  a compression ratio of 1\%. Our tolerance test is described in \rsec{tolerance}, where we compare what happens to the parameter constraints when we introduce noise to elements and eigenvalues separately. Finally, our conclusions are summarised in \rsec{conclusion}.
	
	%Another goal of this work is to explore the shrinkage of the covariance matrices. In \rsec{eigenvalues} and \rsec{snr}, we discard 200 modes of the covariance matrix associated with the highest eigenvalues, then those with lowest signal-to-noise ratio. In \rsec{eigenvalues}, we tested two novel compression scheme and manage to apply a lossless compression scheme capable of reducing the dimension to the number of free parameters, achieving a compression ratio of 1\% for the covariance matrices.
	
	% ----------------------------------------------------------------------
	\section{Methods}
	\label{sec:methods}
	
	\subsection{DES Cosmic Shear: Data and Analysis}
	\label{subsec:data_and_analysis}
	
	In this section, we introduce the data and covariance matrices that are used in this work. Our tests are carried out using cosmic shear statistics $\xi_\pm(\theta)$, focusing on the Year 1 results of the Dark Energy Survey \citep{Troxel:2017xyo,Abbott:2018cms} (DESY1). The data is divided into four tomographic redshift bins spanning the interval $0.20 < z < 1.30$, which yields 10 bin-pair combinations, each one containing 20 angular bins between 2.5 and 250 arcmin. We thus begin with 200 data points for each $\xi_+(\theta)$ and $\xi_-(\theta)$, giving 400 in total. We then apply the angular cuts described in \citep{Abbott:2018cms}, which removes the scales most sensitive to baryonic effects; this leaves 167 points for $\xi_+(\theta)$ and 60 for $\xi_-(\theta)$, resulting in 227 data points corresponding to the aforementioned $227 \times 227$ covariance matrix. 
	
	Table~\ref{tab:priors} shows the 16-parameters varied and the priors placed on them. To perform cosmological parameter inference we use the {\tt CosmoSIS} \citep{Zuntz:2015med, Lewis2000taj, Kirk2012mnras, Kilbinger2009aa, Howlett2012jcap, Bridle2007njp, Takahashi2012taj, Smith2003mnras} pipeline, while employing the {\tt MultiNest} \citep{Feroz:2009fhb} sampler to explore the parameter space, with 1000 {\tt livepoints}, {\tt efficiency} set to 0.05, {\tt tolerance} to 0.1 and {\tt constant efficiency} set to True.
	
	\begin{table}
		\centering
		\begin{tabular} { l c} 
			\hline
			\hline
			Parameter							& Prior	\\ \hline
			Cosmological & \\ [1ex]
			$\Omega_m$						& $\mathcal{U}(0.1, 0.9)$		\\
			$\log A_s$					& $\mathcal{U}(3.0, 3.1)$		\\
			$H_0 \mathrm{(km s^{-1} Mpc^{-1})}$	& $\mathcal{U}(55, 91)$		\\
			$\Omega_b$						& $\mathcal{U}(0.03, 0.07)$	\\
			$\Omega_\nu h^2$					& $\mathcal{U}(0.0005, 0.01)$	\\
			$n_s$							& $\mathcal{U}(0.87, 1.07)$	\\ [1ex]
			\hline
			Astrophysical & \\ [1ex]
			$A_{\mathrm{IA}}$								& $\mathcal{U}(-5, 5)$ \\
			$\eta_{\mathrm{IA}}$							& $\mathcal{U}(-5, 5)$ \\ [1ex]
			\hline
			Systematic & \\ [1ex]
			$m^i$							& $\mathcal{G}(0.012, 0.023)$	 \\
			$\Delta z^1$						& $\mathcal{G}(-0.001, 0.016)$	 \\
			$\Delta z^2$						& $\mathcal{G}(-0.019, 0.013)$	 \\
			$\Delta z^3$						& $\mathcal{G}(0.009, 0.011)$	 \\
			$\Delta z^4$						& $\mathcal{G}(-0.018, 0.022)$	 \\ [1ex]
			\hline
			\hline
		\end{tabular}
		\caption{List of the priors used in the analysis for parameter constraints ($\mathcal{U}$ denotes flat in the given range and $\mathcal{G}$ is gaussian with mean equal to its first argument and dispersion equal to its second). For the cosmological parameters, we fix $w = -1.0$, $\Omega_k =  0.0$ and $\tau =  0.08$. The astrophysical parameters are associated with the intrinsic alignment, they follow the relation $A_{\mathrm{IA}}(z) = A_{\mathrm{IA}}[(1+z)/1.62]^{\eta}$. Lastly, for systematics we have $m^i$ corresponding to the shear calibration and  $\Delta z^i$ for the source photo-$z$ shift, with $i = 1, 4$ in both cases.}
		\label{tab:priors}
	\end{table}

\newcommand\full{Full}	
	The covariance matrices used are the following: \scott{Let's change CL to \full\ or something like that}
	\begin{itemize}
		\item the full covariance matrix used in the DESY1 anaylsis, which includes non-gaussian effects and super-sample variance; it was generated by {\tt Cosmolike} \citep{Krause:2016jvl} (\full);
		\item one containing only the gaussian part, which we will refer to as the Gaussian Covariance Matrix (GCM). %, obtained by running the same code used to analyse the KiDS-450 survey \citep{Kohlinger:2017sxk}, using DESY1 parameters and tomographic bins. We refer to it 
	\end{itemize}
	Thus, throughout, the covariance labels \full\ and GCM differ for several reasons: first, they are two independent codes (GCM is generated by the same code used to analyse the KiDS-450 survey \citep{Kohlinger:2017sxk})
	and, second, although the code for the KiDS-450 survey does contain all the functionality in {\tt Cosmolike}, we ran with the simplest settings in order to accentuate the differences. The ensuing larger differences will help us assess various validation techniques. Where not otherwise stated, the analysis and constraints will be performed on the \full\ covariance matrix.
	
	\rf{Y1-constraints_wmS8A} shows the results for the projected cosmological constraints for \full\ and GCM, using the same data vector and cuts. The $2\sigma$ constraints are as follows: for \full: $\Omega_m= 0.306^{+ 0.073}_{- 0.060}$, $A_{\mathrm{IA}} = 0.852^{+ 1.005}_{- 1.086}$ and $S_8 = 0.784^{+ 0.200}_{- 0.171}$; and for GCM: $\Omega_m = 0.309^{+ 0.073}_{- 0.058}$, 	$A_{\mathrm{IA}} = 0.948^{+ 0.916}_{- 0.985}$ and $S_8 = 0.787^{+ 0.196}_{- 0.166}$. This shows that the differences we have introduced to the calculation of the two matrices are measurable in the parameter constraints.
	
	\Sfig{Y1-constraints_wmS8A}{Constraints on cosmological parameters $\Omega_m$ and $S_8$ and intrinsic alignment parameter $A_{\mathrm{IA}}$ for two covariance matrices produced for cosmic shear. The purple curve is for \full\ while the blue is for GCM. In the 16--dimensional parameter space, the volume of the posterior is about $22\%$ larger for the former.}
	
	% -------------------
	\subsection{Eigenvalues}
	\label{subsec:eigenvalues}
	
	Let us start with the easy task of analysing the eigenvalues of the covariance matrix. Each eigenvalue is associated with a linear combination of the data vector, or a \emph{mode}.% , and it is possible that identifying the modes that have the most discrepant eigenvalues will give guidance on how to reconcile differences.  We plot the eigenvalues for these matrices in \rf{coveigen}. At a first glance, both curves show reasonable agreement, with values differing only by an average of $\approx 13\%$.
	
	%\begin{figure}
	%	\includegraphics[width=0.9\columnwidth]{Eigenvalues/Eigenvalues_Y1_BJ-DES.pdf}
	%	\caption{A log plot showing the $227$ eigenvalues of CL (blue) and GCM (orange) covariance matrices. \label{fig:coveigen}}
	%\end{figure}
	
	The lowest eigenvalues correspond to modes with the smallest variance but since they are not normalised, it is unclear how this variance compares to the signal in the mode. Let us nonetheless explore the possibility that the modes with the lowest variance provide the most information and therefore dropping the ones with the largest eigenvalues would not affect the final result.
	
	Our procedure consists of first diagonalising the covariance matrix in order to calculate its eigenvalues and then replacing the large eigenvalues with a larger number (nine orders of magnitude higher), thus removing their effective contribution; we then transform back to the original basis and perform a cosmological analysis with the new covariance matrix, to constrain the parameters of our model. 
	
	In order to reduce the covariance matrix to about 10\% its original size, we follow the procedure above to discard the 200 eigenmodes with the largest eigenvalues, \rf{EigSNR-constraints_wmS8A} shows the results obtained.  The constraints are significantly broader for the three parameters shown. This is consistent with the fact that we are reducing about 90\% of the information contained in the covariance matrices. However, it is inconsistent with the notion that they are irrelevant, in fact, constraints on $S_8\equiv \sigma_8 (\Omega_m/0.3)^{0.5}$ for the original covariance matrix are $0.784^{+ 0.200}_{- 0.171}$, whereas, for this procedure, we obtain $0.679^{+ 0.533}_{- 0.505}$, showing an increase in almost 200\%. It is then clear that a different way of ordering the modes, other than simply looking at the eigenvalues, is called for.	
	
	\Sfig{EigSNR-constraints_wmS8A}{Constraints on cosmological parameters $\Omega_m$, $S_8$ and the intrinsic alignment parameter $A_{\mathrm{IA}}$ for the original CL covariance matrix (in purple) and for the two new covariance matrices obtained in \rssec{eigenvalues} (in blue) and \rssec{snr} (in magenta).}
	
	\Svwide{SNR_cuts200}{Scatter plot for the relation between the signal to noise (SNR)  for each parameter \scott{I switched x and y here because i think it was a typo} (y-axis) against that for the full set of parameters (x-axis). The derivatives are shown with respect to $\Omega_m$ (blue circle), for $S_8$ (orange \textbf{x}) and for the intrinsic alignment parameter $A_{\mathrm{IA}}$ (green triangle). The purple rectangle spreads until the two hundred lowest values of SNR, which corresponds to the values that are modified for parameter constraints.}
	
	% -------------------
	\subsection{Signal-to-noise ratio}
	\label{subsec:snr}
	
	Instead of looking only at the ``noise'' -- or the eigenvalues of the covariance matrix -- a better way to assess the importance of modes is to consider the signal as well. We can define the expected signal-to-noise ratio (SNR) as
	\be
	\left(\frac{S}{N}\right)^2 = \sum_{ij} T_i C^{-1}_{ij} T_j\
	,\ee
	where $T_i$ is the predicted theoretical signal for the $i^{th}$ data point, given a fiducial cosmology, and $C$ is the covariance matrix. If $C$ were diagonal, then the eigenvectors would simply be the data points themselves, and we could estimate the SNR squared expected in each mode by just computing $T_i^2/C_{ii}$. Then we could throw out the modes with the lowest SNR. Since $C$ is not diagonal, we have to first diagonalise it and then order the values. So, we write the expected SNR squared as
	\bea
	\left(\frac{S}{N}\right)^2 %&=& %\sum_{ij} T_i  C^{-1}_{ij} T_j\nonumber\\
	&=& \sum_{i} \frac{v_i^2}{\lambda_i}\
	,\eea
	where $\lambda_i$ are the eigenvalues of the covariance matrix, which is diagonalised with the unitary matrix $U$, and the eigenvectors are 
	\be
	v_i\equiv U_{ij}^T T_j\
	.\ee
	This makes it very clear which modes should be kept and which should be dropped. Modes $v_i$ for which $v_i^2/\lambda_i$ is very small can be discarded. 
	
	After obtaining the SNR for the covariance matrix, we proceed to set the 200 lowest values to seven orders of magnitude lower, which is equivalent to increasing the noise (or decreasing the signal) of these modes. We then obtain a new covariance matrix with the corresponding modified SNR values. 
	The parameter constraints for this method are shown in \rf{EigSNR-constraints_wmS8A}, where we note that only $\Omega_m$ is well constrained (in agreement with those obtained with the original covariance matrix to within a $2\sigma$ interval). The constraining power on $A_{\mathrm{IA}}$ and $S_8$, on the other hand, is very much lost, which suggests that the modes removed do indeed carry relevant information for these parameters.
	
	We can investigate this loss by tweaking our understanding of which modes carry information. The ``signal'' that these modes are ordered by is the amplitude of the data points.  The parameters , however, are sensitive to the shape as well as the amplitude.
	To address this, we can identify the SNR for each parameter individually by taking
	\bea
	\left(\frac{\partial S/\partial p_\alpha}{N}\right)^2 = \sum_{i} \frac{(\partial v_i / \partial p_\alpha)^2}{\lambda_i}\
	,\eea
	where $\partial /\partial p_\alpha$ is the derivative with respect to each parameter. This produces the SNR for each parameter of interest. The importance of this procedure is illustrated in \rf{SNR_cuts200}, which shows the normalised SNR for a given mode on the $x$-axis and the SNR for each parameter for the same mode for $\Omega_m$, $S_8$ and $A_{\mathrm{IA}}$. The shaded region shows the 200 modes excluded in the previous analysis, so we clearly see that there are some low SNR modes there that contain information about the parameters. This is particularly true for the intrinsic alignment parameter $A_{\mathrm{IA}}$, which seems to explain the poor constraints shown in \rf{EigSNR-constraints_wmS8A}. As a result, simply cutting on raw SNR loses constraining power.

	% -------------------
	\subsection{Tomographic Compression}
	\label{subsec:tomographic_compression}
	

	
	This compression method is based on a Karhunen-Lo\'eve (KL) decomposition for the shear power spectrum suggested by \citep{Alonso:2017hhj} and later applied to real space two-point function in \citep{Bellini:2019ssw} for CFHTLens survey. This method generally finds the eigenmode --- in this case, a linear combination of the convergence in different tomographic bins --- with most of the signal-to-noise ratio contribution to the power spectrum, and then transforms the two-point function of this eigenmode into real space.
	

\newcommand\ctot{\mathcal{C}}

	Before delving into the derivation, it is worth summarizing the results. With {\tt CosmoSIS}, we calculate the shear power spectrum $C_{\ell}$ of the convergence $\kappa^i$ in the 4 tomographic bins probed by DES Year 1 with a fiducial cosmology at the best-fit parameters. \scott{This figure should go from $l\simeq 10\rightarrow \sim 2500$; we should also think why the plot has a large S/N at low $\ell$: there is no information there because of finite sky coverage; are you including an $f_{sky}$ factor?} With 4 bins, there are $4\times 5/2=10$ pairs of bins for which we can compute spectra. The left plot in \rf{ClDl}, shows the diagonal elements of the signal part and the noise part of the spectrum. The right-hand panel shows the spectrum of the modes with the largest signal to noise. That is, we identify a mode as $b_{\ell m} = \sum_i w_i \kappa_{\ell m}^i$, where the sum is over the 4 bins with a weighting factor that we will discuss below. For each $\ell$, the right panel shows the top KL-transformed eigenmodes, which we will call $D_{\ell}$. We can see that the first KL mode contains most of the SNR contribution to the power spectrum. However, if we want to recover more information, we also should include the second and the cross mode between the first and second KL-mode.
	
		\begin{figure*}[thbp]
		\includegraphics[width=0.7\columnwidth]{Cl_pst.png}
		\qquad \qquad \qquad
		\includegraphics[width=0.7\columnwidth]{Dl_pst.png}
		\caption{\textbf{Left:} Shear power spectrum of CL. Solid lines are diagonal elements of the signal matrix $S_{\ell}$, and dashed lines are the diagonal elements of noise matrix $N_{\ell}$.
			\textbf{Right:} Signal-to-noise ratio matrix $D_\ell$ of KL-modes of the power spectrum on the left.  \label{fig:ClDl}}
	\end{figure*}


	\scott{This paragraph is not clear; I tried to rewrite but afraid I will mess things up. It might be simplest if you drop the $l$ subscripts throughout as everything is done at fixed $l$ and focus on the tomographic indices, which I think we are calling $i,j$. Also, I changed from $S\rightarrow C$ so things are consistent.} With  {\tt CosmoSIS}, we generate the shear power spectrum $C_{\ell}^{ij}$ of the convergence $\kappa^i_{\ell m}$ for a fiducial cosmology. The cosmology we choose is the best-fit value of the DES Year 1 results for cosmic shear only. With the total power spectrum $\ctot_{\ell}=C_{\ell}+N_{\ell}$, the sum of the signal and shape noise $N_{\ell}$, we can calculate the Karhunen-Lo\'eve (KL) modes for each $\ell$ (so we drop the $\ell$ subscript) )via a general eigenvalue problem 
	\be
	\ctot^{ij} e^j_p = \Lambda_p N^{ij} e^j_p
	.\ee
		The index $p$ in $e_p^i$ corresponds to a KL-mode of $\ctot$. Using Cholesky decomposition, $N = L L^{\dagger}$, the new observable can be expressed as $b_p = e_p^i  L^{-1}_{ij} \kappa_{j}$\scott{is that the right set of indices?}. We should note that $C_{\ell}$ is the angular power spectrum of the weak lensing shear, and $E_{\ell}$ is similar to a transformation of basis for the shear. So, we can now calculate the power spectrum $D_{\ell}$ for the new observable $b_{\ell m}$ 
	\be
	D_{\ell} =\ \langle b_{\ell m} b_{\ell m}^T \rangle \ = E_{\ell} L^{-1} \ctot_{\ell} L^{-1} E^{T}_{\ell}\
	,\ee
	or, if we denote $E_{\ell} N^{-1}$ as $R_{\ell}$ and further denote $U_{\ell}^{ij}=R^i_{\ell} R^j_{\ell}$, we can write the compression in one simple linear combination of the $C_{\ell}$,
	\be
	D_{\ell} = R_{\ell}^i C_{\ell}^{ij} R_{\ell}^j = U_{\ell}^{ij} \ctot_{\ell}^{ij}\
	.\ee
	$U_{\ell}^{ij}$ is the weight on the tomographic bin-pair, which we can later use to compress the two-point functions. We should point out that these KL-modes are uncorrelated, so the power spectrum of the new observable $D_{\ell}$ is a diagonal matrix, with 1+SNR of the corresponding eigenmodes on the diagonal elements. Since the KL-decomposed modes of shear power spectrum are uncorrelated, we can make a compression here by taking only the first one or two modes with the highest SNR. By doing so, we compress ten tomographic bin-pairs to one or two.
	
	We want, however, to eventually compress the two-point function data vector of DESY1, which is measured is real space and related to the angular power spectrum $C_{\ell}$ via
	\begin{equation*}
	\xi_{\pm}^{ij}(\theta) = \int \frac{\ell d \ell }{2\pi}J_{0/4}(\ell \theta) C^{ij}(\ell)\ 
	.\end{equation*}
	In order to use only a linear combination of all the tomographic bins, we need to make sure that the combination is $\ell$-independent, that is to say, the two-point correlation function corresponding to $D_{\ell}$, $\Tilde{\xi}_{\pm}(\theta)$, can be directly calculated from other two-point functions. In \rf{kl-mode}, we show that compression matrices $U^{ij}(\ell)$ are generally $\ell$-independent, which is usually the case for weak lensing shear. Therefore, we have, 
	\bea
	\nonumber\Tilde{\xi}_{\pm}(\theta) &=& \int \frac{\ell d \ell }{2\pi}J_{0/4}(\ell \theta) D(\ell)\\\nonumber
	&=&\int \frac{\ell d \ell }{2\pi}J_{0/4}(\ell \theta) U^{ij})\ell C^{ij}(\ell)\\
	&=&U^{ij}\xi_{\pm}^{ij}(\theta)\
	,\eea
	where $U^{ij}$ is the average $U^{ij}_{\ell}$ weighted by the number of eigenmodes for each $\ell$ that is $2\ell+1$
	\be
	U^{ij} = \frac{\int_{\ell _{\mathrm{min}}}^{\ell _{\mathrm{max}}} d\ell\, (2 \ell +1) U^{ij}_{\ell}}{\int_{\ell _{\mathrm{min}}}^{\ell _{\mathrm{max}}} d\ell\, (2 \ell +1)}\
	.\ee
	We make a more conservative angular cut than the one discussed in \cite{Troxel:2017xyo}, making sure that both $\xi_{\pm}$ are uniform in regard to tomographic combinations. We consider an angular scale for  $\xi_+$ from $7.195'$ to $250.0'$, and for $\xi_-$ from $90.579'$ to $250.0'$. Therefore, for the purpose of exploring the KL-transform, the raw data vector has a length of 190. By shrinking10 tomographic combinations for each angle into 1 KL-mode, the data vector is shrunk to length 19, and so the number of elements in the covariance matrices are reduced by 99\%.
	
	\begin{figure*}[thbp]
		\includegraphics[width=0.80\columnwidth]{epi.png}
		\includegraphics[width=1.02\columnwidth]{Wij.png}
		\caption{\textbf{Left:} Normalised KL-eigenmodes $E_\ell^p$ of the shear power spectrum $C_{\ell}$, the changes in shades represent different $\ell$. \textbf{Right:} Transformation on tomographic bin combination $U_{ij}$ constructed by the KL-eigenmodes. Black lines are the weighted average of each mode. \label{fig:kl-mode}}
	\end{figure*}
	
	%We characterise a given element in the data vector by its angular and tomographic indices: $a_i = a_{\ell m,\alpha}$ where $l,m$ denote the indices corresponding to given spherical harmonics and $\alpha$ is a given tomographic bin, or equivalently $a_i = a_\alpha(\vec\theta)$ in real space. The compression occurs in terms of tomographic bins, so that 
	%\be
	%b_\mu(\vec\theta) = \sum_{\alpha} F_{\mu\alpha} a_\alpha(\theta)
	%\ee
	%where the $F$'s are chosen \footnote{Note that this definition of $F$ differs from that in \citep{Alonso:2017hhj} in that it includes the inverse of the noise matrix.} so that the correlation functions of the $b$'s are diagonal in tomographic space:
	%\bea
	%w_{\mu\nu}(\theta) &=& \langle b_\mu(\vec\theta_1) b_\nu(\vec\theta_2)\rangle\vert_{\vert\vec\theta_1-\vec\theta_2\vert\in\theta}
	%\svs
	% &=& \delta_{\mu,\nu} w_{\mu}(\theta).
	%\eea
	%Usually with $N_t$ tomographic bins, one must consider $N_t(N_t+1)/2$ correlation functions, but -- due to the transformation that diagonalises the elements -- there are only $N_t$ correlation functions to consider (for a given $\theta$). Even better, these can be ordered by the information they contain, so fewer than $N_t$ can be used. In the example used in \citep{Alonso:2017hhj}, 16 tomographic bins were assumed, so that the standard treatment would require 136 correlation functions, but only 3 were needed in order to extract accurate constraints. 
	
	%This method suggests a way of extracting the most important pieces of the full covariance matrix $C$. We simply compute the covariance matrix of the $w_{\mu}$ in terms of $C$ and keep only the most important terms. That is,
	%\bea
	%C^b_{\mu\nu} &\equiv& \langle (w_{\mu} -\bar w_\mu)\,(w_{\nu} -\bar w_\nu)\rangle
	%\svs
	% &=& \sum_{\alpha\alpha'\beta\beta'} F_{\mu\alpha}F_{\mu\alpha'}\, F_{\nu\beta}F_{\nu\beta'}\, C_{\alpha\alpha'\beta\beta'} .
	%\eea
	%Here the angular indices have been suppressed (there are two of them, one for each $w_\mu$), but the full tomographic complexity has been retained. The covariance matrix on the right includes a total of $N_t^2 \times N_t^2$ terms (some of which are equal because of symmetry) corresponding to all possible pairs of two-point functions. But $C^b$ on the right contains only $N_t^2$ elements, and again these are ordered, so we can use only a subset of them. In the simplest case, where only one linear combination is needed so $\mu=\nu=1$, a single number (for each pair of angular bins) captures all the relevant information from the full covariance matrix.
	
	
	In \rf{kl-mode}, we plot the normalised KL-eigenmode $E_\ell^i$ of $C_{\ell}$ and its corresponding $U^{ij}_\ell=R_\ell^i R_\ell^j$. Modes with increasing $\ell$ are plotted in increasing darkness of the colour. We can see that the KL-modes do not depend significantly on the scale factor $\ell$, so we also take the weighted average of the eigenmodes $E_\ell^p$ and its quadratic form $U_\ell$ over $\ell$'s and plot them with black lines. 	We see that for different $\ell$, the KL-modes do vary by a slight amount. For the first KL-mode, the tomographic bins with higher redshift are weighted more than those with low redshift. This is also shown in the right panel by the weight on tomographic combination that the combination of bin 3 and bin 4 carries most of the weight in the signal-to-noise ratio. This agrees with the fact that low-redshift galaxies are less affected by lensing than high-redshift galaxies, as indicated in the left panel of Figure \ref{fig:ClDl}.
		
	\begin{figure}[b]
		\includegraphics[width=\columnwidth]{CompKL-constraints_wmS8A.pdf}
		\caption{Cosmological constraints marginalised over all 16 parameters for the  $190 \times 190$ CL covariance matrix and that compressed by the first KL-mode and the first two KL-modes.\label{fig:CompKL-constraints_wmS8A}}
	\end{figure}

% 	\begin{figure}
% 	\includegraphics[width=0.8\columnwidth]{corr_diag_kl.pdf}
% 	\caption{The correlation matrix of Gaussian (upper right) and CL (bottom left) covariance matrices, and their difference (bottom right) in the elements \label{fig:kl_corr}}
% 	\end{figure}
	
	We ran the likelihood analysis with the first KL-mode and the first two KL-modes with their cross correlation mode, which correspond to a 10-to-1 and 10-to-3 compression, respectively, and show the parameter constraints on the $\Omega_m - S_8 - A_{\mathrm{IA}}$ plane in Figure \ref{fig:CompKL-constraints_wmS8A}. We can see that the first KL-mode is generally not sufficient to recover the information in the data vector. Since the first two modes contain most of the SNR contribution at a map level, we were able to recover the $\Omega_m$ constraints. However, information about the $S_8-A_{\mathrm{IA}}$ combination is clearly lost. This could be due to the fact that the SNR-prioritised modes are not the sensitive direction for these parameters, as was also the case in \rf{SNR_cuts200}. Indeed, the $S_8 - A_{\mathrm{IA}}$ plane shows a strong correlation between these two parameters. This likely explains why the $S_8$ constraints got wider: the KL-modes fail to break the degeneracy on $A_{\mathrm{IA}}$, which is mostly contained in the modes that are insensitive to cosmic shear, and are discarded in the compression process.

	% -------------------
	\subsection{Linear combinations of the data vector}
	\label{subsec:2pt_compression}
	\scott{Isn't this moped? If so, we should rename the section heading and the references.}
	
	\Svwide{Weights_2pt}{An illustration of the 227 values of the weights corresponding to $\Omega_m$, $S_8$ and $A_{\mathrm{IA}}$ used for compressing the covariance matrices. Note how similar the weighing vectors for $S_8$ and $A_{\mathrm{IA}}$, and that the largest values for correspond to the last 60 elements, i.e. these will be used to compress the part of the covariance matrix that holds information for $\xi_-$.}
	
	The compression here takes place at the two-point level \citep{Zablocki:2015zcm}, with the compressed data vector containing linear combinations of the many two-point functions. In principle, this requires only $N_p$ linear combinations of the two-point functions where $N_p$ is the number of free parameters, and each mode, or linear combination, contains all the information necessary about the parameter of interest. 
	
	For each parameter $p_\alpha$ that is varied, one captures a single linear mode
	\be
	y_\alpha = U_{\alpha i} D_i\
	,\ee
	where $D_i$ are the data points and the coefficients are defined as
	\be \label{eq:compression_scheme}
	U_{\alpha i} \equiv \frac{\partial T_j}{\partial p_\alpha} \, C^{-1}{}_{ji}\
	,\ee
	with $T_j$ being the theoretical prediction for the data point $D_j$ for a fiducial cosmology. An illustration of the matrix $U_{\alpha i}$ is shown in \rf{Weights_2pt}, showing the weighting vector for parameters $\Omega_m$, $S_8$ and $A_{\mathrm{IA}}$.
	
	The now much smaller data set $\{y_\alpha\}$, which contains $N_p$ data points, carries its own covariance matrix, with which $\chi^2$ can be computed for each point in parameter space. Propagating through shows that this covariance matrix is related to the original $C_{ij}$ via
	\be
	C_{\alpha\beta} = U_{\alpha i} C_{ij} U^T_{j\beta}\ 
	,\ee
	which also happens to be identical to the Fisher matrix of our likelihood. This type of compression was first suggested by \citeauthor{Tegmark:1997maa} (1997), and is the basis for the MOPED algorithm \citep{Heavens:2000hjl}, in which the original procedure was extended for multiple parameters. Note that our weighing vector given by \ec{compression_scheme} does not carry the normalising factor used in the aforementioned paper, nor are the $U_\alpha$'s independent of each other. We find that this does not, however, deteriorate the analysis.\scott{I don;t think i understand this.}
	
	In our case, the covariance matrix is  $227 \times 227$, while the number of parameters needed to specify the model is only 16, so $C_{\alpha\beta}$ is a $16\times 16$ matrix. We have apparently captured from the initial set of $(227 \times 228)/2 = 25,878$ independent elements of the covariance matrix a small subset (only 136) of linear combinations of these 26k elements that really matter. If two covariance matrices give the same set of $C_{\alpha\beta}$, it should not matter whether any of the other thousands of elements differ from one another.
	
	Ultimately, what matters is how well the likelihood does at extracting parameter constraints. Since most analyses assume a Gaussian likelihood, this boils down to how well the contours in parameter space agree when computing $\chi^2$ using two different covariance matrices.	
	
	\rf{Comp2pt-constraints_wmS8A} compares the constraints obtained for the compressed covariance matrix and data set with results from the full one. The two curves agree extremely well for the parameters shown: $\Omega_m$, $S_8$ and $A_{\mathrm{IA}}$. This is also true for all the other cosmological and intrinsic alignment parameters, where their mean values agree at the $2 \sigma$ confidence level. While the volume of the whole constrained parameter space does increase by about 13\%, the constraints for most parameters are less than 4\% broader, which shows that the information loss is negligible. 
	
	\Sfig{Comp2pt-constraints_wmS8A}{Constraints on cosmological parameters $\Omega_m$ and $S_8$ and for the intrinsic alignment parameter $A_{\mathrm{IA}}$ for the original CL covariance matrix (in purple) and for the compressed one (in blue).}
	
%	One relevant point in this analysis is at which point to take the derivative of each parameter. When we wish to compare the results of our compression scheme with those obtained with the full covariance matrix and data set, it is important to derivate each parameter at their respective mean value (obtained by performing the analysis with the full covariance matrix). The shape and variance of the posterior is not dependent on the derivative, but the best-fit value shifts according to the point where the derivative is taken.

	% ----------------------------------------------------------------------
	\section{Comparison of Covariance Matrices}
	\label{sec:comparison_matrices}
	
	
	Armed with this information about compression, we now set out to compare the two covariance matrices, GCM and \full, described in \S\ref{subsec:data_and_analysis}. 
	
	\subsection{Element-by-element comparison}
	\label{subsec:compare_one-one}
	
	We begin by performing an element-by-element comparison between the two covariance matrices. If there were only a single data point, then the covariance matrix would be one number and comparing two covariance matrices to try to understand why they give different constraints would be as simple as comparing these two numbers.  The simplest generalisation is then to do an element-by-element comparison. We make a scatter plot of the elements of the two matrices in the bottom panel of \rf{Y1-scatter}, where we can see that the elements of CL are, in general, larger than GCM's, with many of the off-diagonal elements differing by 2 orders of magnitude or more.
In some ways, this is useful and reassuring, as it aligns with what we see in the parameter constraints, in \rf{Y1-constraints_wmS8A}: larger elements in the covariance matrix translates to less constraining power. 
	
	The limitation of this method is that it remains unclear which of the differences are driving the final discrepancies in parameter constraints. This difficulty is an outgrowth of the increasing size of the data sets and hence the growing number of elements of the covariance matrix that any two codes are likely to disagree on. This element-by-element comparison, however, would prove much more useful if we fewer elements to compare. Towards that end, we turn to compressed covariance matrices.
	%This element-by-element comparison, however, would prove much more useful if we could first determine the important elements. Towards that end, we start by turning to the eigenvalues.
	
	\Sfig{Y1-scatter}{In both plots, the red points refer to the diagonal elements, and the colour bar varies according to the number of elements in one hexagonal bin, where the darkest blue colour corresponds to only one element, and the brightest yellow shade to 2000. \textbf{Top:} Scatter plot of the ratio of the elements of CL and GCM vs the GCM value. For illustrative purposes, we draw a black, horizontal line at CL/GCM = 1. \textbf{Bottom:} Density of the scatter plot of the positive elements of the GCM and CL, with the black line showing CL = GCM.}
	
	% -------------------
	\subsection{Compressed Matrices Comparison}
	\label{subsec:compare_compressed}
	
	\subsubsection{Using MOPED}
	
	\begin{figure}[b]
		\sfig{Comp2pt-scatter}{0.85\columnwidth}
		\caption{{\small Results for the covariance matrices compressed following the procedure described in \rssec{2pt_compression}, with the red points corresponding to the diagonal elements.
		\textbf{Top:}  One-to-one scatter of the ratio of the elements of CL and GCM, over elements of CL. The black horizontal line is drawn at CL/GCM = 1.
		\textbf{Bottom:}  One-to-one scatter of the elements of the compressed matrices, with the black line describing CL = GCM.}}
		\label{fig:Comp2pt-scatter}
	\end{figure}
	
	We compress both covariance matrices using the same $U_{\alpha, i}$ (we also tried using different $U$'s for each and obtained similar results).
	%Here, we take two different approaches: first, we assume that $U_{\alpha, i}$ is the same for both covariance matrices and we calculate it with CL. The second approach is that each compression scheme must use the original covariance matrix that will be compressed, so that $U_{\alpha, i}$ will be different for each covariance matrix. We find that the mean values of the parameter constraints for the two methods both agree to $> 2\sigma$ of the original constraints. It is also crucial that the matrices used for comparison here are those obtained via the same compression scheme, so that we can be sure that their differences are indeed only related to the differences in the original matrices. 
	Fig.~\rf{Comp2pt-scatter} show a one-to-one scatter plot, which, as expected, exhibits a similar behaviour to that observed in \rf{Y1-scatter}, with the elements of \full\ being larger than those of GCM. Here, however, the ratio of the diagonal elements is closer to 1, and the ratio of the diagonal elements goes up to only $\approx 2.3$. Perhaps even more importantly, there are much fewer points on this plot, meaning that MOPED reduces the number of elements that need to be compared. These figures provide a greater insight into the relevant elements for parameter estimation: the dispersion is largely damped, and most of the elements are within $25\%$ of each other, which explains what we see in the parameter constraints. Fig.~\rf{Comp2pt-correlation} shows the correlation matrix for GCM and \full, and the difference between the normalized off-diagonal elements. The small differences suggest that the root of the slightly looser constraints obtained with GCM is the larger diagonal elements of the MOPED-reduced covariance matrix. That is, a problem that initially required inspecting hundreds of thousands of elements is reduced to one involving only 16.
	
	\Sfig{Comp2pt-correlation}{The upper right and lower left plots display the correlation matrix for GCM and CL respectively, and the difference between them, $\Delta r_{ij}$, is shown on the lower right.}
	
	% ---------
	\subsubsection{Using map-level compression}
	In \rf{comp-cov}, we plot the compressed covariance matrices for the CL and GCM with the first mode only, and show a one-to-one comparison of the covariance matrices. By comparing the bottom panel of  \rf{Y1-scatter} with the left panel of \rf{comp-cov}, we notice that the large regions containing the elements with greater difference are now gone. Instead, the two covariance matrices just have a relative constant difference, because of the fact that we did not include non-gaussian effect in one of them. This shows that the divergence between CL and GCM does not considerably affect the overall SNR. 
	
	\begin{figure}[thbp]
	\includegraphics[width=0.8\columnwidth]{kl_scatter.png}
	\caption{ \textbf{Top:} One-to-one scatter of the ratio of CL over GCM and the CL elements. \label{fig:comp-cov}   \textbf{Bottom:} One-to-one scatter of the two compressed matrices following the procedure described in \ref{subsec:tomographic_compression}. }
	\end{figure}
	
	% ----------------------------------------------------------------------
	\section{Tolerance of the Compressed Matrices}
	\label{sec:tolerance}
	
	\Svwide{Tolerance_constraints}{An error plot showing the changes to the constraints for $\Omega_m$, $S_8$ and $A_{\mathrm{IA}}$ for errors added at $5\%, 10\%, 15\%, 25\%, 30\%, 35\%, 40\%$ and $45\%$ of the original elements (in purple) and eigenvalues (in green) of the compressed covariance matrix. The blue rectangle covers the $2 \sigma$ interval obtained for the original CL covariance matrix, and the green line shows the mean value for the respective parameter. }
	
	Now that we have shown that we are indeed able to compress the covariance matrix into a much simpler and considerably smaller one, our next step is to analyse the amount of error the elements can tolerate while reproducing compatible parameter constraints. 
	
	In the next two sections we test two different ways of perturbing the covariance matrix: first we consider an error to the elements themselves, then we follow a similar procedure to study the effects of introducing error to the eigenvalues of the compressed covariance matrix. In both cases the perturbation is drawn in the following manner: consider that we want to test the impact of an error $x \%$; this can either be an increase of a decrease in the original element, or eigenvalue, such that what we care about most is not whether the parameter constraints will be larger, but rather how different. For this error to be random, but centred at our desired percentage, we draw $\delta$ from a Gaussian distribution, $\mathcal{G}(0,\frac{x}{100})$ and calculate the new value to be
	\be \label{eq:tolerance}
	C_{\alpha \beta}^{\mathrm{new}} = (1 + \delta)C_{\alpha \beta}^{\mathrm{old}}\ 
	,\ee
	where, for the eigenvalue, we replace $C_{\alpha \beta}$ with $\lambda_i$. This analysis is done only for the CL covariance matrix, with errors ranging from $5 - 45 \%$, and for 50 realizations of the perturbed matrices.
	
	One of the concerns that arises when modifying the covariance matrix is that the resulting one has to be positive definite (PD), as such, in each section we also describe the steps taken to ensure this.
	
	%One of the issues that arises when arbitrarily modifying the elements of the covariance matrix is that the new one does not necessarily remain positive definite. In this analysis, we take an extra step to ensure that this characteristic is retained, as we will see below.
	
	% -------------------
	\subsection{Modifying the elements}
	
	Once we generate new values for each independent element, following \ec{tolerance}, we check for positive definiteness. Since the resulting matrix is, more often than not, not PD, we correct this by identifying the smallest negative eigenvalue and adding it to the diagonal \cite{Yuan:2008}. We check that, although doing this largely increases the values of the diagonal elements, only less than $40 \%$ have a standard deviation of more than twice the original perturbation. 
	
	The constraints for $\Omega_m$, $S_8$ and $A_{\mathrm{IA}}$ are shown in Fig. \rf{Tolerance_constraints}, in purple, where the blue rectangle spans over the constraints for the unchanged compressed covariance matrix. It is interesting to note that, while the relative change in size for the $2\sigma$ interval is $> 8 \%$ for the cosmological parameters, this goes up to $\sim 80 \%$ for the intrinsic alignment parameter $A$.
	
	%We see that errors of up to 25\% translate to $< 10\%$ difference in the constraints. A $30\%$ error, on the other hand, shows differences of up to $33\%$ and $24\%$, respectively. It is worth noting here that we also find that $S_8$ is less sensitive to these noise introduced.
	
	%\begin{figure}[thbp]
	%	\includegraphics[width=\columnwidth]{Tolerance_constraints.pdf}
	%	\caption{An error plot showing the changes to the constraints for $\Omega_m$ and $S_8$ for errors added at $5\%$, 10\%, 15\%, 25\% and 30\% of the original elements (in black) and eigenvalues (in blue) of the compressed covariance matrix. The green rectangle covers the $2 \sigma$ interval obtained for the original CL covariance matrix, and the green line shows the mean value for the respective parameter. } %\label{fig:tolerance}
	%\end{figure}
	
	% -------------------
	\subsection{Modifying the eigenvalues}
	
	Another way of introducing error to the covariance matrix is to perturb its eigenvalues. For a symmetric matrix, we have
	\be
	C_{\alpha \beta} = Q\Lambda Q^{-1}\ 
	,\ee
	where $\Lambda = \lambda I$, with $\lambda$ being the eigenvalues and $I$ the identity matrix; and $Q$ is a square matrix whose columns are composed of the eigenvectors of $C_{\alpha \beta}$. The eigenvalues are then perturbed as described in \ec{tolerance}, and the error, $\delta$ is drawn from $\mathcal{G}(0,\frac{x}{100})$, with the requirement that $|\delta| < 1$. We then have $\lambda^{\mathrm{new}} > 0$, and thus the perturbed covariance matrix associated with these new eigenvalues is PD. 
	
	The results for this method are also plotted in Fig. \rf{Tolerance_constraints}, in green. Despite the results following the same tendency as those of the last section, we find that about $80\%$ of the elements of the perturbed covariance matrices are within $10\%$ of their original value. This shows that the intrinsic alignment parameter $A$ is very sensitive to errors in the covariance matrix.
	
	% ----------------------------------------------------------------------
	\section{Conclusion}
	\label{sec:conclusion}
	
	In this work, we set out to explore different ways of compressing, comparing and analysing covariance matrices, giving particular emphasis to MOPED. We started off looking at the parameter constraints of two $227 \times 227$ covariance matrices CL and GCM, generated for DESY1 cosmic shear measurements, and saw that, although some of their elements differed by several orders of magnitude, they generated similar constraints. It was clear, then, that not all elements contribute equally to the parameter constraints, and we needed to employ increasingly complicated methods to try and locate the most relevant parts of the covariance matrix.
	
	The first step was then to analyse the eigenvalues. We began with the hypothesis that modes associated with the lowest eigenvalues have the lowest variance and therefore carry most information, as such, those with the highest eigenvalues would contribute less to parameter estimation. Using this notion to compress the covariance matrix yielded worse constraints: ``removing" the highest 200 eigenvalues, by setting them to nine orders of magnitude higher resulted in a loss of about 200\% on the constraining power. Next, we moved on to the signal-to-noise ratio, and, using a similar procedure adopted for the eigenvalues, we ``removed" the modes with the lowest SNR. The results showed us that these modes did not contribute significantly to constraining some cosmological parameters, like $\Omega_m$, but constraints on the intrinsic alignment parameters, and even $S_8$ were considerably affected. This is consistent with the fact that the IA parameters are more sensitive to low SNR in cosmic shear, and it shows us that we need to look at the SNR per parameter before making any cuts, so that we do not lose important information for the parameters that we want to constrain.
	
	The next step was to shrink the covariance matrix by applying a tomographic compression, where we decompose the shear power spectrum into KL modes, then we look for those with the highest SNR. We thus go from ten tomographic bins to only one or two. The resulting covariance matrix, for one mode, is then reduced from $190 \times 190$ to $19 \times 19$ or $59 \times 59$, showing a reduction of about $99\%$ or $91\%$, respectively. We show, however, that one mode is not sufficient for constraining the parameters of our model, with the results being similar to our previous tests involving SNR: the constraints for $\Omega_m$, for example, are reproduced with the first and second KL-mode, but this is not the case for the IA parameters. Since essential information of IA parameters is contained in low SNR KL-mode, the high KL-modes failed to break the degeneracy of $A_{\mathrm{IA}}-S_8$ correlation, resulting in wider $S_8$ constraints. 
	
	Finally, we applied MOPED, which uses linear combinations of the data vector. By transforming the data vector and covariance matrix with a weighting vector that is parameter dependent, we were able to reduce the $227 \times 227$ matrix to a $16 \times 16$, and since the Fisher matrix is identical for both the original and compressed ones, the compression scheme is lossless. This is also clear in the parameter constraints, where we show that we are able to reproduce the similar constraints for the two matrices, for all parameters. On the other hand, we compared the elements of the compressed covariance matrix for CL and GCM and found that the new elements show reasonable agreement, with their correlation matrices being very similar, and the diagonal elements showing a percentage difference of less than $15\%$.
	
	When looking at \rf{Y1-scatter}, the large variance in the element-by-element comparison suggests that there could be considerable differences in the parameter constraints. We see, however, in \rf{Y1-constraints_wmS8A}, that this is not the case. This becomes clearer when comparing the elements of the compressed covariance matrices, where, while they do follow the same tendency as the full comparison, only a smaller portion of the elements display a greater dispersion.
	
	One last step was taken to analyse the error tolerance of the compressed CL covariance matrix. We adopted two ways of doing this, by introducing error taken from a Gaussian distribution for $5 - 45 \%$ of the original 1) element and, 2) eigenvalue of the compressed covariance matrix. For the latter, we checked that only about $20 \%$ of elements of the resulting, perturbed, covariance matrix showed errors within the expected value, while the vast majority had only about a $10\%$ error. In both cases, however, the results were similar: for the cosmological parameters $\Omega_m$ and $S_8$, the $2\sigma$ constraints changed by less than $8\%$, while the constraining power for intrinsic alignment parameter $A_{\mathrm{IA}}$ was largely lost. This result is repeated throughout this work: we find that most modifications made to the covariance matrix greatly affect $A_{\mathrm{IA}}$, with the exception of MOPED. It is, therefore, advised that extra precaution be taken when performing compression methods, if this parameter is to be considered.
	
	% ----------------------------------------------------------------------
	% ----------------------------------------------------------------------
	\subsection*{Acknowledgments}
	
	The author wish to thank Sukhdeep Singh and Hung-jin Huang for useful discussions.
	
	%%% Here is where you should add your specific acknowledgments, remembering that some standard thanks will be added via the \code{desc-tex/ack/*.tex} and \code{contributions.tex} files.
	
	%This paper has undergone internal review in the LSST Dark Energy Science Collaboration. % REQUIRED if true
	
	\input{contributions} 
	
	T.F. and T.Z. contributed extensively writing the main paper as well as implementing the covariance comparison and compression. N.C. contributed to the compression code. All authors participated in the discussion and gave valuable suggestions.
	
	% Standard papers only: author contribution statements. For examples, see http://blogs.nature.com/nautilus/2007/11/post_12.html
	
	% This work used TBD kindly provided by Not-A-DESC Member and benefitted from comments by Another Non-DESC person.
	
	% Standard papers only: A.B.C. acknowledges support from grant 1234 from ...
	
	\input{desc-tex/ack/standard} % also available: key standard_short
	
	%	\input{Appendix}
	
	% ----------------------------------------------------------------------
	% ----------------------------------------------------------------------
	
	
	% This work used some telescope which is operated/funded by some agency or consortium or foundation ...
	
	% We acknowledge the use of An-External-Tool-like-NED-or-ADS.
	
	%{\it Facilities:} \facility{LSST}
	
	% Include both collaboration papers and external citations:
	\bibliography{main}
	
\end{document}

% ======================================================================

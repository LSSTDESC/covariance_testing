\RequirePackage{docswitch}

\documentclass{article}
\usepackage{lsstdesc_macros}
\usepackage{xcolor}
\usepackage{indentfirst}
\usepackage{hyperref}
\usepackage[margin=1.5in]{geometry}

\newcommand\scott[1]{{\bf {\textcolor{teal}{[Scott: #1]}}}}
\newcommand\tassia[1]{{\bf {\textcolor{teal}{[Tassia: #1]}}}}
\newcommand\tq[1]{{\bf {\textcolor{teal}{[Tq: #1]}}}}
\newcommand\reply[1]{{\bf {\textcolor{teal}{[#1]}}}}
\newcommand\done{{\bf {\textcolor{olive}{[Done]}}}}



\begin{document}

    {\LARGE\textbf{Review by internal referees}}\\ \\ \\

    {\Large\textbf{Second Reports}}\\
    
    \textbf{Alan Heavens, August 4}\\

    With apologies for the delay in getting comments to you, here goes.  This draft is much improved over the first one, and the focus is clearer.  I think in particular the comparison between the field-level (KL-like) compression and summary-stats level (MOPED) is interesting.  The field-level is not the most general one, but if I understood correctly, is Davide’s elegant weighting of tomographic fields.  I am still not convinced of the motivation for the large eigenvalue/small S/N compressions, and I guess the results rather bear that out, but I don’t see much harm in testing some ‘straw person’ cases.  It is possible that a referee will take a different view.\\
 
    \underline{Abstract}
    
    The covariance matrix is a four-point function only if the statistics are two-point functions. \done

    It wasn’t immediately clear to me how compression helps identify the significant parts of the covariance matrix.  Does this use the magnitude of the elements in the matrix which translates the original covariance matrix to the compressed one?  This would make sense. \reply{Exactly.}

    In the last line, would ‘inaccuracies’ be more suitable than ‘changes’? \done\\

    \underline{Introduction}
    
    In the first paragraph, the analysis only requires a covariance matrix if the likelihood is assumed to be gaussian.  This is a big assumption (and not necessarily always good).  A bit more explanation is needed here. \reply{We reworded this to reflect the more general case.}

    MOPED is not ‘truly’ lossless, but only in the restricted sense that the Fisher matrix of the compressed data is the same as for the full dataset, if certain conditions are met.  For a given data set, this usually makes the likelihood surface near the peak almost identical, and in practice the similarity often extends right into the wings of the distribution, as you find here in Fig. 8. \reply{Fixed this.}

    In the list of applications, can you add Mootoovaloo et al. (\href{https://arxiv.org/pdf/2005.06551.pdf}{arXiv:2005.06551}) for an application to weak lensing?  It is the one I mentioned was coming, in my earlier report. \done\\

    \underline{Section II B}
    
    Louca and Sellentin (\href{https://arxiv.org/pdf/2007.07253.pdf}{arXiv:2007.07253}) actually argue for removing the highest S/N modes (to reduce biases).  This is worth referring to. \reply{We did a similar analysis removing the 200 modes with highest SNR and found similar results. We chose not to display those here so as not to produce plots with confusing information. We point out, however, that this might have been different if we had not removed such a large number of modes. We added a paragraph referring to this.}

    “almost 200\%” $\rightarrow$ “the errors of almost 200\%”. \done\\

    \underline{Section II C}
    
    The sentence saying that the eigenvalues would be the data points themselves doesn’t make sense to me.  Do you mean they would be the diagonal elements themselves? \reply{We mean that the eigenvectors would simply be the $T_i$s (or on the data the $D_i$’s); i.e., they would not be linear combinations of either.}

    After (3), “Modes...can be discarded” rather pre-empts the conclusion (that in fact they can’t, without losing information).  I’d reword this so it doesn’t suggest that they can. \done\\

    \underline{Section II D}
    
    It is worth pointing out that the linear compression is not the most general one, since the weights don’t depend on $l$ or $m$.  As Davide found, it can be very effective nonetheless. \done

    Typo: existance $\rightarrow$ existence. \done\\

    \underline{Section II E}
    
    (After equation 12). In TTH we only considered compression for one parameter for the ‘known covariance matrix’ case.  The extension from one to more than one is not as trivial as it appears (for example, we suggested a recipe for this for the KL ‘known mean’ case, and in fact it is often a very poor solution).  The MOPED paper showed that all elements of the Fisher matrix could be reproduced simultaneously, and not just a single element, or just the diagonal elements.  I’d suggest rewording to ‘This compression was first suggested by Tegmark et al. for a single parameter only’.  The non-trivial extension to multiple parameters, where the full Fisher matrix is reproduced with the compressed data, is the MOPED algorithm.’ \done\\
    
    \underline{Section III B}
    
    In the top panel of Fig. 10, it would be helpful to plot GCM/FCM, so the top plot resembles more the bottom, rather than being flipped. \reply{We made the changes also to Fig. 9, which has a similar plot.}\\
    
    \underline{Section IV}
    
    I think I mentioned this before, but one way to ensure that C is positive definite is to perturb the matrix log of C.  It’s worth perhaps offering it as an alternative, and giving the argument (I think you had one) for why you don’t do it this way. \done \\
    
    \underline{Section V}
    
    In ‘The next step was to shrink...’ I don’t think you decompose the shear power spectrum, but rather the data vector.  i.e. you do this at the field level (albeit its spherical harmonic transform).  1-point, rather than 2-point. \done

    You might also want to mention in connection with Harry Mootoovaloo’s paper, that to speed up the analysis one needs a fast way to make theoretical predictions for the compressed data (you don’t generally want to generate full data sets and do repeated linear compression, since that is slow, and sometimes as slow as analysing the full data).  Harry used Gaussian processes to generate the compressed theory. \reply{We commented on this in Section II E.} \\  \\
	
	
	\textbf{David Alonso, April 4} \\

    I thank the authors for the revised manuscript. The paper reads much better, and its main results and conclusions are much clearer and useful. Just a couple of minor points:
    \begin{itemize}
        \item I may have missed this in the text (in which case I apologize): is Fig. 12 showing the results for one of the 50 realizations of the covariance matrix? Or the mean of the constraints for all of them? \reply{We generate 50 different covariance matrices with the desired error budget, then take the mean covariance out of those. The parameter constraints are performed only for that covariance matrix.}
        \item Do you have any intuition why $A_{\mathrm{IA}}$ seems to be more sensitive to covariance errors than any other parameters? I understand why this is the case in the context of KL transforms, but not in general. \\  \\
    \end{itemize}
	
	
	\textbf{Jonathan Blazek, August 16} \\

    My apologies to the authors for this delayed review. I thank the authors for their work in addressing my first round of comments, and I agree with the other reviewers that the paper has been significantly improved. I don't have much to add to the other comments, but here are a few things: 
    \begin{itemize}
        \item Related to David's question, it would be great if a bit more insight could be provided into why the $A_{\mathrm{IA}}$ parameter is so sensitive. I suspect this is related to the fact that the covariance has no IA contribution, but I'm not sure how to best test this. In any event, it must be the case that the sensitivity to noise in the elements/eigenvectors is related to the sensitivity to low S/N modes - perhaps there is a simple mathematical way to make this connection.
        \item line 12: “ways of compression” $\rightarrow$ “compression methods”. \done
        \item line 539: loss of 200\% of constraining power. This wording is confusing. \reply{We meant that the $2 \sigma$ intervals increased by about $200\%$.}
        \item Sec IV: (Related to one of Alan's points above) It's not entirely clear (to me) why this method of perturbing the compressed cov matrix is a realistic test of how errors would arise in actual applications. Could this be motivated a bit more? \reply{We believe it may be more applicable when dealing with the compressed covariance matrices themselves. There have been various works where they are used, and people are more and more motivated to finding ways to speed up the compression (like the Gaussian Processes in Mootoovaloo's paper). In that sense, our work shows how well you can constrain the cosmological parameters with a compressed covariance that is different from what it would be, if one were to use the full analysis for finding the fiducial values.}
    \end{itemize}

    
    \pagebreak

    {\Large\textbf{First Reports}}\\
	
	\textbf{Alan Heavens, March 22} \\
	
	As I see it, the paper has two broad aims: to explore a number of ways to compress weak lensing data; and to look at the sensitivity of results to covariance matrix errors, including seeing which elements of the covariance matrix are most important for parameter inference, and which should therefore be computed especially accurately if possible.
	
	The paper makes a number of good points, but it has some problems, and this report is rather critical, but I hope constructively so.  It might be worth stepping back and thinking what the main messages of the paper should be, what results can be presented that are genuinely new, and if some of the studies can be more systematic than now.  \\
	
	\underline{Summary}
	
    The executive summary is that the compression side tries a few things, and concludes that the best way is to use MOPED.  Since MOPED is known to be optimal and can’t be beaten in terms of the Fisher matrix (subject to some conditions), this shouldn’t be a surprise.  For suggestions of how to present something new here, see details below.
	
	For the covariance matrix studies, this is an interesting question to tackle.  The concern I have is that it’s not clear how to do this in a systematic way, and I think that a referee will consider it to be too ad hoc to draw general conclusions from.  This needs to be strengthened.  I suggest a small technical improvement to avoid non-positive definite matrices, but I don’t have any good ideas for a more systematic general approach. \\
	
	\underline{Detailed comments}
	
	The first compression method involves modifying the highest or lowest eigenvalues of the covariance matrix, effectively discarding the eigenmodes with the highest or lowest eigenvalues.  It’s perhaps not surprising that this doesn’t work very well, since the procedure knows nothing about the sensitivity of the covariance matrix to the parameters, and there is no formal way of ordering the modes by contribution to the information on each parameter. 
	The second compression method is better, based around S/N eigenmodes (Vogeley and Szalay 1996 should be referenced). Here linear combinations of the field values are considered (field here corresponds to shear measurements on a number of tomographic slices). Compression to the highest S/N modes is optimal in one sense, if (and only if) it is the amplitude of the (various) power spectra that is the (single) target.  This is a more principled approach, and it is not surprising that it does better.  It is still not very good, as Fig. 4 shows, because in general, we want to infer multiple parameters, whose influence on the power spectra is more complicated.  This problem was tackled by Tegmark, Taylor, Heavens (1997;  TTH), and we found the modes that would be optimal for any single parameter, by including the sensitivity of the power spectra to the parameters. However, the challenge is to find a compression that works for all parameters simultaneously.  This is not at all trivial. In TTH we proposed combining the optimal modes determined for each parameter, and it turns out to offer something, but in the end it does not work very well, for reasons that I can explain if needed.   The S/N eigenmodes do seem to be quite effective, in a restricted setting when applied in the radial direction only, as Davide Alonso found.  Here we see that they are limited in this context, perhaps because once again they have no knowledge of the sensitivity of the signal to the parameters, so are therefore almost inevitably sub-optimal.
	
	The final approach is to work with linear combinations of the two-point functions (rather than of the field).  This uses a principled approach, and is the MOPED algorithm from Heavens, Jimenez, Lahav (2000), just without the (optional) Gram-Schmidt orthogonalization.  A number of authors have rediscovered it since then, such as Gualdi et al., but the radical compression to the number of parameters goes back to HJL.  TTH found the solution for a single parameter, but (as seen above in the compression of field values) the extension to multiple parameters is not trivial at all; guaranteeing that the Fisher matrix is unchanged on compression is not even obviously possible.  What HJL showed mathematically is that in fact it is, and the credible regions can be unchanged (at least at the level of the Fisher approximation), even with such a radical compression, and this is what Fig.11 basically shows.  This of course is wonderful, but I wonder if it tells us anything new – we already know that MOPED works (and that there is nothing that can do better for gaussian data in terms of Fisher matrix).
	
	To present something new here, it might be best to angle it as the first analysis that applies MOPED to a weak lensing situation with $>15$ parameters.   In the interest of full disclosure, we have a paper in late draft stage that applies MOPED to weak lensing data.  Given the LSST internal review process, our paper will likely appear on arxiv before this one.  However, it has fewer parameters, so you might want to stress the $>15$ parameters aspect.
	\reply{We thank you for this information. We have included this point in the paper; we also make it clear that we're using MOPED (minus the normalising factor).}
	
	A suggestion for the covariance matrix study, to ensure that the perturbed matrix is positive definite, is to take the matrix log of the fiducial covariance matrix, and then to modify the elements of the log matrix in some way (how to do this systematically, I’m not sure).  Taking the matrix exponential will then give a positive-definite matrix for sure. It’s still a bit ad hoc, but avoids that particular problem. 
	\reply{One of the issues here is how to introduce an error to the log matrix that would be similar to what we expect to see in the original matrix. A naive assumption would be that 
	$$C^{new}_{\alpha \beta} = (1 + \delta)C^{old}_{\alpha \beta}$$
	$$\rightarrow log \left( C^{new}_{\alpha \beta}\right) = log(1 + \delta) + log \left( C^{old}_{\alpha \beta} \right)\ .$$ We know this to be wrong, since obtaining the matrix logarithm is not as simple as that, so this approach would be incorrect. Also, applying error to the elements of the log matrix (like a $10\%$ error) produces a covariance matrix with elements several orders of magnitude larger than the original one. This would work if we diagonalised the matrix and applied the transformation to the $log$ of its eigenvalues, but since we already have a section for modifying the eigenvalues, we fear this would be repetitive.}\\

	\underline{Smaller points are:}
	
	The comparison of the elements in Fig. 2 is a bit misleading (or rather, the conclusions are).  Assuming that the small elements (which show the biggest fractional difference) are off-diagonals, then it probably doesn’t much matter if they are $10^{-4}$ or $10^{-8}$ of the diagonal elements – they are effectively zero.  If they are on the diagonal, then it’s a different matter, so it is worth checking. \reply{We added a more thorough investigation into these differences, pointing out that the biggest difference are in the smallest elements and correspond to off-diagonal terms.}
	
	The compression in Fig. 8 shows that for the compressed data, the two covariance matrices lead to compressed covariance matrices which agree much better.  This is encouraging, and expected since the linear combinations involved will be dominated by the larger elements, which agree closely, and differences in the small elements are largely irrelevant. \reply{These are the results we were hoping for.}
	
	For the MOPED compression, it is suggested that the full covariance matrix is used to determine the fiducial model where the derivatives are computed, but this is slow. It is much faster to choose a rough fiducial model and find the maximum likelihood from a preliminary MOPED analysis and then repeat with this as the fiducial model.  The process could be repeated as many times as necessary, but it’s never been needed in my experience. \reply{The fiducial values used were the bestfit values obtained in the previous analysis with the full covariance matrices. We agree with your comments, but we already had those results at hand.}
	
	I’m sorry not to be able to suggest a route that makes the covariance matrix study likely to be strong enough, but I hope that these comments are constructive. \reply{These points have helped immensely, and we are very thankful. We hope that we have now been able to provide a draft with clearer and more conclusive results.}\\  \\
	
	
	\textbf{David Alonso, April 3} \\
	
	My overall comments would go along the same lines as Alan's, so I won't repeat them here. My main worry is the fact that there doesn't seem to be a clear “lesson learnt” or “take-home message” with regards to the problem of identifying the most relevant components of the covariance matrix. This is a hard problem! One possible angle that I didn't see explored here is whether MOPED would be able to circumvent errors in the covariance matrix. I.e. how much do the contours change if you use the two different covariances? How about their associated minimum $\chi^2$? \reply{We explored this in more detail with the addition of the constraints on the intrinsic alignment parameter $A_{\mathrm{IA}}$. We generated 9 new covariance matrices by introducing error to the compressed Full Covariance Matrix (FCM, which we previously referred to as CL), and saw that, while the constraints on $\Omega_m$ and $S_8$ did not alter by much, there was an alarming loss of constraining power for $A_{\mathrm{IA}}$}
	
	Beyond this, I have a number of specific comments. Here they are in order:
	\begin{itemize}
	\item \textbf{L32} Individual $\rightarrow$ independent? \done
	\item \textbf{L43} I'd remove this paragraph from the introduction. \done
	\item \textbf{L47} You seem to use the present continuous a lot “we are trying”. I think I'd go for the present simple. \done
	\item \textbf{L113} Could you explain in a bit more detail what the differences are between the two covariances? I.e. is it that one is missing the non-Gaussian parts? \reply{We explain this better in the text now. The Gaussian Covariance Matrix (GCM) only consists of the Gaussian parts.}
	\item \textbf{Eq. 1} Does $T$ here contain the noise bias contribution? I imagine not, but I wanted to check. \reply{This is just the formal definition.}
	\item In both sections IV and V you seem to arbitrarily choose 200 as the number of modes to throw away. Can you justify this? \reply{We modified the text to make this clearer. This is done to achieve a reduction of about $90\%$ of the covariance matrix, so we have a “fairer” comparison between these methods and the other, more complicated compression schemes.}
	\item \textbf{Fig. 5} This would benefit from larger labels and legend. You could even consider making it a double-column figure. \done
	\item \textbf{L249} “compression at the ell space” - I think I'd say “map-level compression”. \done
	\item \textbf{Eq. 6} Use $\langle$ and $\rangle$ instead of $<$ and $>$. \done
	\item \textbf{L314} It is worth justifying that ell-independent $U^{ij}$ is a good approximation (which it usually is for cosmic shear). \reply{Low ell spread is because of the cosmic variance, and is also lower weighted by the weighted average, high ell shows great consistency in Figure 5.}
	\item \textbf{Eq. 10} It'd be good to justify the 2ell+1 weighting. \reply{Justified by the number of modes for each ell.}
	\item \textbf{Fig. 7} I think $W_{ij}$ should be $U^{ij}$ in the caption. \done
	\item \textbf{L354} is $\rightarrow$ are \done
	\item \textbf{L354} The result also makes sense in terms of the cumulative nature of lensing (i.e. the further away the source, the more lensed it is), although the thinning out of the galaxy density at higher redshifts makes the 4th bin have a lower weight than the 3rd one. \done
	\item \textbf{Fig. 8} This plot is tiny! Do you need the right panel? \done
	\item \textbf{Fig. 10} Seems to appear in the text before Fig. 9. Actually, I'm not sure you discuss figure 9. \reply{This was removed.}
	\item \textbf{Eq. 12} The transformation is parameter-dependent. I assume you perform it at a fiducial cosmological model? \reply{We do indeed. We used the best-fit results from the full cosmological analysis and take those to be our fiducial cosmology everywhere.}
	\item \textbf{Eq. 13} Probably worth emphasizing that this is the Fisher matrix of your likelihood. \done
	\item \textbf{L415} Where does “eighty thousand” come from? \reply{We were referring to the elements of the covariance matrix. We fixed this in the text.}
	\item “The shape and variance of the posterior is not dependent on the derivative, but the best-fit value shifts according to the point where the derivative is taken.” I'm a bit confused by this. Can you justify this statement? \reply{We've removed this paragraph from the paper. The point of it was to explain that we needed a cosmological analysis had to be done first, in order to get the bestfit values.}
	\item You should try to reorganize the figures so they appear a bit closer to where they are discussed. \done
	\item \textbf{L449} “We find that the mean values of the parameter constraints for the two methods agree to $1 \sigma$, which shows that they are equivalent to each other.” I don't understand this statement in the context in which it appears (the comparison between covariance matrices). \reply{Since we are comparing the two compressed covariance matrices, there are two ways of doing this: we can either compress them using the same weight (remember that the inverse of the covariance matrix is used for this result), or with a different weight for each (the weight would be obtained with the corresponding covariance matrix). We wanted to check how different weights impact the parameter constraints and found the differences to be small (as compared to the difference between the constraints for the two matrices).}
	\item \textbf{L496} The qualitative jump between $25\%$ and $30\%$ errors is hard to digest. What is the cause of this? Also, did you do this for a single realization of the noisy covariance or for several of them? \reply{We redid the analysis with 50 realizations of the covariance matrix. The “jump” was most likely due to a large fluctuation, which was corrected with more realizations. The new analysis brought new aspects to light, which are pointed out in the new draft.}
	\item \textbf{L510} Do you mean $\delta > -1$? \reply{We meant $|\delta| < 1$.} \\
	\end{itemize}


	\textbf{Jonathan Blazek, April 6} \\
	
	I have followed with interest the progress of this important work, and I congratulate the authors on completing the paper. Please let me know if you have questions about the below comments/suggestions. Note that I deliberately wrote my comments before reading the other reviews, so there is some overlap.
	
	One general comment before specifics. The paper has two main goals: 1) comparing different covariance codes/methods and 2) exploring data compression. I find that these two threads are not always clearly distinguished. For instance, much of the data compression is done only on the CL covariance matrix, and the conclusions about the comparison part are not given much attention. It could help to slightly restructure where the results are presented, or to add a few words making clear when CL-only is being used and when an important comparison is being done.
	
	\begin{itemize}
	\item For the reason mentioned above, the abstract should be more clear. What are the main goals and conclusions? Also “start along the path” in the first sentence sounds tentative for the start of an abstract. \reply{Updated the abstract to include more information about our goals and conclusions.}
	\item Reference numbers are out of order. \reply{The references are in alphabetical order, that's why they're not in increasing order in the text.}
	\item \textbf{L34}: “As data sets increase” $\rightarrow$ you mean the number of elements in the data vector, not necessarily the overall amount of observational data. \reply{We replaced \textit{data sets} with \textit{data vectors}.}
	\item \textbf{L68} Missing section symbol. \done
	\item \textbf{L83} Emphasize here (or elsewhere) that this analysis is done in configuration rather than Fourier space. Is there any reason we would expect the qualitative results to be different in a Fourier space analysis? \reply{Map level compression actually works better in Fourier space, but we don't think MOPED is going to differ much in two setups. The reason we use real space is because it is in line with the DES Y1 analysis.}
	\item \textbf{Table 1} Here, and in the relevant parts of the text, it may improve clarity to add the $_{IA}$ subscript to the A and eta variables. Also, you can cite the Troxel DES cosmic shear paper when mentioning the IA params. \done
	\item \textbf{L84} DES Y1 is the state-of-the-art for cosmic shear data sets, at least right now. You could mention this.
	\item \textbf{L97} Are all of these citations for CosmoSIS needed? \reply{These are the references that CosmoSIS spits out after each run. We thought it would be polite to include all of them.}
	\item \textbf{L140} The difference of 4 orders of magnitude is casually discussed without further explanation. Although your later results show that this is mostly due to non-important elements, more explanation at this stage would be helpful. Why is this happening? How can we plausibly think that these two cov matrices are roughly the same when there are such large differences. \reply{We try to address this by taking into consideration Alan's comments as well. The elements that show greatest discrepancies are the smallest ones (of order $10^{-18}$, this is probably why the constraints are not all that different. As can be seen, the diagonal elements (and the larger ones), have smaller differences.}
	\item \textbf{Paragraph beginning at L163} I was a bit lost by the logic of this paragraph. If possible, provide more explanation of what you are doing and why. \reply{We removed the figure and the associated text.}
	\item \textbf{L179 (and elsewhere)} Why 200 eigenmodes? This number is not explained, and the results depend strongly on how many eigenmodes are kept. Similar question about the S/N analysis. \reply{We try to explain this better in the text. This is done to achieve a reduction of about $90\%$ of the covariance matrix, so we have a “fairer” comparison between these methods and the other, more complicated compression schemes.}
	\item \textbf{L183} “Throwing away $90\%$ of the information...” By what metric? Where does this number come from? \reply{Change to reducing the number of elements in the covariance matrix by 90\%.}
	\item \textbf{Fig. 4 (and elsewhere)} The blue/purple/magenta color scheme is tough to parse. They look similar, especially with overlapping contours. \reply{Changed one of the blueish color to green. The goal is trying to be colorblind-friendly as well as monochrome copy friendly.}
	\item \textbf{Fig. 4 caption} You probably don’t need all of the detail on how you remove these modes (which is included in the text anyway). You can just say that you remove those modes. \done
	\item \textbf{L210} “very small.” How is that quantified? \reply{We meant the lowest 200.}
	\item \textbf{L226} “signal” OF these modes... \done
	\item \textbf{L234} “vanilla” What does this mean? \reply{Changed to 'simple'.}
	\item \textbf{L237} Explain that the shaded region shows the modes excluded in the previous analysis. \reply{This is in the text.}
	\item \textbf{L249} “Compression at the ell-space...” This sentence is confusing. \reply{Changed to “map-level compression”.}
	\item \textbf{L267-269} “Eliminates even more modes… the numbers are the same.” These ideas seem to be in contradiction. Which point do you want to emphasize? \reply{This paragraph was removed.}
	\item \textbf{Eq. 5} I don’t understand the indices. What is the summation convention? \reply{They are really just matrix multiplication, simplified to matrix representation.}
	\item \textbf{L298} “double summation” What is meant? \reply{It really just meant $\sum_i$ $\sum_j$.}
	\item \textbf{Eq. 8} This is the standard xi+- expression, which is not how it is described in the text above. \reply{Changed so that it is only meant to be used to derive the $\xi_{+/-}$ for the compressed data.}
	\item \textbf{L323-324} Wrong symbol. Should be arcmin, not deg. \done
	\item \textbf{L342} “increasing shades” is confusing. I also found the plot itself hard to parse, since the contrast between different ell values isn’t very much. \reply{Made it so it is a little easier to see the change in color.}
	\item \textbf{L345} What is $W_\ell$? \reply{It should be $U_\ell$.}
	\item \textbf{L347} You say earlier that the transformation has to be ell independent, but then discuss here that this is some ell dependence. Is this a problem? If not, why not? \reply{For low ells, there is the cosmic variance that is inevitable, but they are lower weighted so it is fine. For higher ells, they converge to an acceptable level.}
	\item\textbf{ L353-354} I found this sentence confusing. \reply{Added more specifications of what the results reveal.}
	\item \textbf{L369} First two KL modes and their cross correlation? \reply{Yes, and changed.}
	\item \textbf{L415} Do you mean elements or linear combinations of the elements? \reply{We meant the elements.}
	\item \textbf{L428} This discussion of parameter space volume is tough to understand without discussing the number of parameters.  Also, do you want to claim that the other parameters agree well when they are only consistent at the 2sigma level? \reply{We wanted to be very conservative here. The cosmological and IA parameters show an agreement of $> 99\%$ (with the exception of $\alpha_{\mathrm{IA}}$). The real problem are the systematic parameters.}
	\item \textbf{Fig. 7} As I mentioned before, the different colors are pretty subtle and hard to parse. Also, you should mention the ell range spanned by the different color shades. \reply{Changed the color to blue-green-orange, should be better. Also specify the meaning of different shade level in the caption.}
	\item \textbf{Fig. 8} Larger text, like in Fig 12. Also, why are some of the diag elements below 1, when all of the others are above? I didn’t see a discussion of this anywhere. \reply{Specified in Section III A in v2.}
	\item \textbf{L443} Extra space in i.e. \done
	\item \textbf{L451} Agreeing to 1sigma does not imply equivalence. \reply{Changed this.}
	\item \textbf{A general comment about figures} There are several similar figures, and they don’t always appear right next to where they are described in the text. So it would help if each caption included info on which method of compression is being used. \done
	\item \textbf{Fig. 9} Where in the text is this figure mentioned? \reply{We removed this figure.}
	\item \textbf{Section VII} Is this always with the CL covariance? Is there any tolerance testing for the other one? \reply{The tolerance testing is done only on CL/FCM. We believe the results will be similar, if done with GCM.}
	\item \textbf{L475} Clarify that the eigenvalues here are not the original eigenvalues used in the compression method. \done
	\item \textbf{L479} What is this “extra step” which is mentioned a couple of places? Is it simply a check on the final cov matrix? If so, does this introduce a selection bias on the otherwise random noise in the cov? \reply{We made this clearer in the text. For the elements, a constant is added to the diagonal and in the case of the eigenvalues, $|\delta| > 1$.}
	\item \textbf{L517} What is the takeaway of this result and this final sentence? The question of noise in the eigenvalues is just dropped. What does this result mean? \reply{We have given more emphasis in this new draft.}
	\item \textbf{Conclusion} Say more about the results of the comparison between the two methods. What have you learned? \reply{We included a discussion about this in the new version.}
	\item \textbf{L538} Is it actually untrue that these modes carry less information, or is it simply that they do carry some information? \reply{We see that they do carry relevant information, since we lost constraining power for all the parameters by applying this cut (this wasn't the case for SNR, for instance).}
	\item \textbf{L549} The phrasing of IA being sensitive to low S/N may be misleading. You mean low S/N in cosmic shear — the cov doesn’t include IA at all. \done
	\item \textbf{L559} “consecutive” $\rightarrow$ “resulting”? \done
	\item \textbf{Fig. 14} It’s hard to see what’s going on since we don’t know how the x axis is ordered. \reply{We added labels to the x-axis, showing the parts corresponding to $\xi_+$ and to $\xi_-$.}
	\item \textbf{Fig. 15} Hard to see black vs blue lines. \reply{We replace the figure, and changed the colours.}
	\item \textbf{Fig 15} “englobes” $\rightarrow$ “covers”. \done
	\item \textbf{L577} This is very interesting - is it truly lossless? \reply{We included more information regarding this: it is lossless only if the noise in the data does not depend on the model parameters.}
	\item \textbf{L592} The sudden degradation in constraining power when going from $25\%$ to $30\%$ errors is surprising (to me, at least) and an interesting result. Are you able to explain what is going on? \reply{The results shown in the paper had been for only one realization of the perturbed covariance matrix. We redid the analysis for 50 of those, and the results are now different -- the previous results were likely due to statistical fluctuations.}
	\item \textbf{L595}: What is happening with the eigenvalue noise? Are there interesting lessons from this result? \reply{Introducing noise to the eigenvalues was done in an attempt to ensure positive definiteness of the covariance matrix. The constraints obtained are similar to those obtained when the perturbation is applied to the elements themselves. The interesting lesson is what happens to the $A_{\mathrm{IA}.}$: even though the error in the final elements is small, the constraining power is largely lost for this parameter.}

\end{itemize}

\end{document}
